2023-05-26 21:21:42,714 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.16 (default, Mar  2 2023, 03:21:46) [GCC 11.2.0]
CUDA available: True
GPU 0: GRID A100X-20C
CUDA_HOME: None
GCC: gcc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0
PyTorch: 1.10.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.11.0
OpenCV: 4.7.0
MMCV: 1.4.4
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.3
MMSegmentation: 0.20.2+609cf9d
------------------------------------------------------------

2023-05-26 21:21:42,715 - mmseg - INFO - Distributed training: False
2023-05-26 21:21:43,235 - mmseg - INFO - Config:
norm_cfg = dict(type='SyncBN', requires_grad=True)
model = dict(
    type='EncoderDecoder',
    pretrained='pretrain/mit_b5.pth',
    backbone=dict(
        type='MixVisionTransformer',
        in_channels=3,
        embed_dims=64,
        num_stages=4,
        num_layers=[3, 6, 40, 3],
        num_heads=[1, 2, 5, 8],
        patch_sizes=[7, 3, 3, 3],
        sr_ratios=[8, 4, 2, 1],
        out_indices=(0, 1, 2, 3),
        mlp_ratio=4,
        qkv_bias=True,
        drop_rate=0.0,
        attn_drop_rate=0.0,
        drop_path_rate=0.1),
    decode_head=dict(
        type='SegformerHead',
        in_channels=[64, 128, 320, 512],
        in_index=[0, 1, 2, 3],
        channels=256,
        dropout_ratio=0.1,
        num_classes=16,
        norm_cfg=dict(type='SyncBN', requires_grad=True),
        align_corners=False,
        loss_decode=dict(
            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),
    train_cfg=dict(),
    test_cfg=dict(mode='whole'))
dataset_type = 'CustomDataset'
data_root = '/ofo-share/repos-david/data/mmseg-training/safeforest23'
classes = ('Dry Grass', 'Green Grass', 'Dry Shrubs', 'Green Shrubs', 'Canopy',
           'Wood Pieces', 'Litterfall', 'Timber Litter', 'Live Trunks',
           'Bare Earth', 'People', 'Sky', 'Blurry', 'Obstacles')
img_norm_cfg = dict(
    mean=[71.27821354460872, 70.71086780697692, 71.91041988796658],
    std=[32.98343261843338, 31.74368632485595, 32.66905186610359],
    to_rgb=True)
crop_size = (256, 512)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(848, 480), ratio_range=(0.5, 2.0)),
    dict(type='RandomCrop', crop_size=(256, 512), cat_max_ratio=0.75),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(
        type='Normalize',
        mean=[71.27821354460872, 70.71086780697692, 71.91041988796658],
        std=[32.98343261843338, 31.74368632485595, 32.66905186610359],
        to_rgb=True),
    dict(type='Pad', size=(256, 512), pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(848, 480),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[71.27821354460872, 70.71086780697692, 71.91041988796658],
                std=[32.98343261843338, 31.74368632485595, 32.66905186610359],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='CustomDataset',
        data_root='/ofo-share/repos-david/data/mmseg-training/safeforest23',
        img_dir='img_dir/train',
        ann_dir='ann_dir/train',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations'),
            dict(type='Resize', img_scale=(848, 480), ratio_range=(0.5, 2.0)),
            dict(type='RandomCrop', crop_size=(256, 512), cat_max_ratio=0.75),
            dict(type='RandomFlip', prob=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(
                type='Normalize',
                mean=[71.27821354460872, 70.71086780697692, 71.91041988796658],
                std=[32.98343261843338, 31.74368632485595, 32.66905186610359],
                to_rgb=True),
            dict(type='Pad', size=(256, 512), pad_val=0, seg_pad_val=255),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_semantic_seg'])
        ],
        img_suffix='_rgb.png',
        seg_map_suffix='_segmentation.png',
        classes=('Dry Grass', 'Green Grass', 'Dry Shrubs', 'Green Shrubs',
                 'Canopy', 'Wood Pieces', 'Litterfall', 'Timber Litter',
                 'Live Trunks', 'Bare Earth', 'People', 'Sky', 'Blurry',
                 'Obstacles')),
    val=dict(
        type='CustomDataset',
        data_root='/ofo-share/repos-david/data/mmseg-training/safeforest23',
        img_dir='img_dir/val',
        ann_dir='ann_dir/val',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(848, 480),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[
                            71.27821354460872, 70.71086780697692,
                            71.91041988796658
                        ],
                        std=[
                            32.98343261843338, 31.74368632485595,
                            32.66905186610359
                        ],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        img_suffix='_rgb.png',
        seg_map_suffix='_segmentation.png',
        classes=('Dry Grass', 'Green Grass', 'Dry Shrubs', 'Green Shrubs',
                 'Canopy', 'Wood Pieces', 'Litterfall', 'Timber Litter',
                 'Live Trunks', 'Bare Earth', 'People', 'Sky', 'Blurry',
                 'Obstacles')),
    test=dict(
        type='CustomDataset',
        data_root='/ofo-share/repos-david/data/mmseg-training/safeforest23',
        img_dir='img_dir/test',
        ann_dir='ann_dir/test',
        img_suffix='_rgb.png',
        seg_map_suffix='_segmentation.png',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(848, 480),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[
                            71.27821354460872, 70.71086780697692,
                            71.91041988796658
                        ],
                        std=[
                            32.98343261843338, 31.74368632485595,
                            32.66905186610359
                        ],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        classes=('Dry Grass', 'Green Grass', 'Dry Shrubs', 'Green Shrubs',
                 'Canopy', 'Wood Pieces', 'Litterfall', 'Timber Litter',
                 'Live Trunks', 'Bare Earth', 'People', 'Sky', 'Blurry',
                 'Obstacles')))
log_config = dict(
    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = True
optimizer = dict(
    type='AdamW',
    lr=6e-05,
    betas=(0.9, 0.999),
    weight_decay=0.01,
    paramwise_cfg=dict(
        custom_keys=dict(
            pos_block=dict(decay_mult=0.0),
            norm=dict(decay_mult=0.0),
            head=dict(lr_mult=10.0))))
optimizer_config = dict()
lr_config = dict(
    policy='poly',
    warmup='linear',
    warmup_iters=1500,
    warmup_ratio=1e-06,
    power=1.0,
    min_lr=0.0,
    by_epoch=False)
runner = dict(type='IterBasedRunner', max_iters=20000)
checkpoint_config = dict(by_epoch=False, interval=2000)
evaluation = dict(interval=2000, metric='mIoU', pre_eval=True)
work_dir = './work_dirs/segformer_mit-b5_512x512_20k_safeforest_over_canopy'
gpu_ids = range(0, 1)

2023-05-26 21:21:43,236 - mmseg - INFO - Set random seed to 115300502, deterministic: False
2023-05-26 21:21:43,974 - mmseg - INFO - initialize MixVisionTransformer with init_cfg {'type': 'Pretrained', 'checkpoint': 'pretrain/mit_b5.pth'}
2023-05-26 21:21:44,877 - mmseg - INFO - initialize SegformerHead with init_cfg {'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
Name of parameter - Initialization information

backbone.layers.0.0.projection.weight - torch.Size([64, 3, 7, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.0.projection.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.0.norm.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.0.norm.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.norm1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.norm1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.attn.attn.in_proj_weight - torch.Size([192, 64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.attn.attn.in_proj_bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.attn.attn.out_proj.weight - torch.Size([64, 64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.attn.attn.out_proj.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.attn.sr.weight - torch.Size([64, 64, 8, 8]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.attn.sr.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.attn.norm.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.attn.norm.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.norm2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.norm2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.ffn.layers.0.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.ffn.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.ffn.layers.1.weight - torch.Size([256, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.ffn.layers.4.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.ffn.layers.4.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.norm1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.norm1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.attn.attn.in_proj_weight - torch.Size([192, 64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.attn.attn.in_proj_bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.attn.attn.out_proj.weight - torch.Size([64, 64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.attn.attn.out_proj.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.attn.sr.weight - torch.Size([64, 64, 8, 8]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.attn.sr.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.attn.norm.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.attn.norm.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.norm2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.norm2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.ffn.layers.0.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.ffn.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.ffn.layers.1.weight - torch.Size([256, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.ffn.layers.4.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.ffn.layers.4.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.norm1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.norm1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.attn.attn.in_proj_weight - torch.Size([192, 64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.attn.attn.in_proj_bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.attn.attn.out_proj.weight - torch.Size([64, 64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.attn.attn.out_proj.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.attn.sr.weight - torch.Size([64, 64, 8, 8]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.attn.sr.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.attn.norm.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.attn.norm.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.norm2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.norm2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.ffn.layers.0.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.ffn.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.ffn.layers.1.weight - torch.Size([256, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.ffn.layers.4.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.ffn.layers.4.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.0.projection.weight - torch.Size([128, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.0.projection.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.0.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.0.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.norm1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.norm1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.attn.attn.in_proj_weight - torch.Size([384, 128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.attn.attn.in_proj_bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.attn.attn.out_proj.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.attn.attn.out_proj.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.attn.sr.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.attn.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.attn.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.norm2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.norm2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.ffn.layers.0.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.ffn.layers.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.ffn.layers.1.weight - torch.Size([512, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.ffn.layers.4.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.ffn.layers.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.norm1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.norm1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.attn.attn.in_proj_weight - torch.Size([384, 128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.attn.attn.in_proj_bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.attn.attn.out_proj.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.attn.attn.out_proj.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.attn.sr.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.attn.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.attn.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.norm2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.norm2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.ffn.layers.0.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.ffn.layers.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.ffn.layers.1.weight - torch.Size([512, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.ffn.layers.4.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.ffn.layers.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.norm1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.norm1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.attn.attn.in_proj_weight - torch.Size([384, 128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.attn.attn.in_proj_bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.attn.attn.out_proj.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.attn.attn.out_proj.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.attn.sr.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.attn.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.attn.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.norm2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.norm2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.ffn.layers.0.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.ffn.layers.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.ffn.layers.1.weight - torch.Size([512, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.ffn.layers.4.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.ffn.layers.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.norm1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.norm1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.attn.attn.in_proj_weight - torch.Size([384, 128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.attn.attn.in_proj_bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.attn.attn.out_proj.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.attn.attn.out_proj.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.attn.sr.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.attn.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.attn.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.norm2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.norm2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.ffn.layers.0.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.ffn.layers.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.ffn.layers.1.weight - torch.Size([512, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.ffn.layers.4.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.ffn.layers.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.norm1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.norm1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.attn.attn.in_proj_weight - torch.Size([384, 128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.attn.attn.in_proj_bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.attn.attn.out_proj.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.attn.attn.out_proj.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.attn.sr.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.attn.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.attn.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.norm2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.norm2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.ffn.layers.0.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.ffn.layers.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.ffn.layers.1.weight - torch.Size([512, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.ffn.layers.4.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.ffn.layers.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.norm1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.norm1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.attn.attn.in_proj_weight - torch.Size([384, 128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.attn.attn.in_proj_bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.attn.attn.out_proj.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.attn.attn.out_proj.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.attn.sr.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.attn.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.attn.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.norm2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.norm2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.ffn.layers.0.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.ffn.layers.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.ffn.layers.1.weight - torch.Size([512, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.ffn.layers.4.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.ffn.layers.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.0.projection.weight - torch.Size([320, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.0.projection.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.0.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.0.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.0.projection.weight - torch.Size([512, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.0.projection.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.0.norm.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.0.norm.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.attn.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.attn.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.attn.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.attn.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.ffn.layers.0.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.ffn.layers.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.ffn.layers.1.weight - torch.Size([2048, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.ffn.layers.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.ffn.layers.4.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.ffn.layers.4.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.attn.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.attn.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.attn.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.attn.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.ffn.layers.0.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.ffn.layers.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.ffn.layers.1.weight - torch.Size([2048, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.ffn.layers.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.ffn.layers.4.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.ffn.layers.4.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.attn.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.attn.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.attn.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.attn.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.ffn.layers.0.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.ffn.layers.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.ffn.layers.1.weight - torch.Size([2048, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.ffn.layers.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.ffn.layers.4.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.ffn.layers.4.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.conv_seg.weight - torch.Size([16, 256, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.conv_seg.bias - torch.Size([16]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.convs.0.conv.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.convs.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.convs.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.convs.1.conv.weight - torch.Size([256, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.convs.1.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.convs.1.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.convs.2.conv.weight - torch.Size([256, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.convs.2.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.convs.2.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.convs.3.conv.weight - torch.Size([256, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.convs.3.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.convs.3.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.fusion_conv.conv.weight - torch.Size([256, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in ConvModule  

decode_head.fusion_conv.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.fusion_conv.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  
2023-05-26 21:21:44,893 - mmseg - INFO - EncoderDecoder(
  (backbone): MixVisionTransformer(
    (layers): ModuleList(
      (0): ModuleList(
        (0): PatchEmbed(
          (projection): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
          (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
        )
        (1): ModuleList(
          (0): TransformerEncoderLayer(
            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
              (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): TransformerEncoderLayer(
            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
              (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (2): TransformerEncoderLayer(
            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
              (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      )
      (1): ModuleList(
        (0): PatchEmbed(
          (projection): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        )
        (1): ModuleList(
          (0): TransformerEncoderLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): TransformerEncoderLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (2): TransformerEncoderLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (3): TransformerEncoderLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (4): TransformerEncoderLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (5): TransformerEncoderLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      )
      (2): ModuleList(
        (0): PatchEmbed(
          (projection): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        )
        (1): ModuleList(
          (0): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (2): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (3): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (4): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (5): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (6): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (7): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (8): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (9): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (10): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (11): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (12): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (13): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (14): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (15): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (16): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (17): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (18): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (19): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (20): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (21): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (22): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (23): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (24): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (25): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (26): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (27): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (28): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (29): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (30): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (31): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (32): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (33): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (34): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (35): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (36): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (37): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (38): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (39): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      )
      (3): ModuleList(
        (0): PatchEmbed(
          (projection): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        )
        (1): ModuleList(
          (0): TransformerEncoderLayer(
            (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): TransformerEncoderLayer(
            (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (2): TransformerEncoderLayer(
            (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  init_cfg={'type': 'Pretrained', 'checkpoint': 'pretrain/mit_b5.pth'}
  (decode_head): SegformerHead(
    input_transform=multiple_select, ignore_index=255, align_corners=False
    (loss_decode): CrossEntropyLoss()
    (conv_seg): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))
    (dropout): Dropout2d(p=0.1, inplace=False)
    (convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (1): ConvModule(
        (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (2): ConvModule(
        (conv): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (3): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
    )
    (fusion_conv): ConvModule(
      (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activate): ReLU(inplace=True)
    )
  )
  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
)
2023-05-26 21:21:44,914 - mmseg - INFO - Loaded 32 images
2023-05-26 21:21:53,145 - mmseg - INFO - Loaded 11 images
2023-05-26 21:21:53,147 - mmseg - INFO - Start running, host: exouser@dev-07-derek-9-image-david, work_dir: /ofo-share/repos-david/mmsegmentation_mine/work_dirs/segformer_mit-b5_512x512_20k_safeforest_over_canopy
2023-05-26 21:21:53,147 - mmseg - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2023-05-26 21:21:53,147 - mmseg - INFO - workflow: [('train', 1)], max: 20000 iters
2023-05-26 21:21:53,147 - mmseg - INFO - Checkpoints will be saved to /ofo-share/repos-david/mmsegmentation_mine/work_dirs/segformer_mit-b5_512x512_20k_safeforest_over_canopy by HardDiskBackend.
2023-05-26 21:22:16,928 - mmseg - INFO - Iter [50/20000]	lr: 1.955e-06, eta: 2:37:37, time: 0.474, data_time: 0.163, memory: 3384, decode.loss_ce: 1.3992, decode.acc_seg: 8.0313, loss: 1.3992
2023-05-26 21:22:36,614 - mmseg - INFO - Iter [100/20000]	lr: 3.940e-06, eta: 2:23:54, time: 0.394, data_time: 0.152, memory: 3384, decode.loss_ce: 1.4026, decode.acc_seg: 26.1661, loss: 1.4026
2023-05-26 21:22:56,863 - mmseg - INFO - Iter [150/20000]	lr: 5.916e-06, eta: 2:20:21, time: 0.405, data_time: 0.154, memory: 3384, decode.loss_ce: 1.2704, decode.acc_seg: 31.2296, loss: 1.2704
2023-05-26 21:23:17,100 - mmseg - INFO - Iter [200/20000]	lr: 7.881e-06, eta: 2:18:23, time: 0.405, data_time: 0.153, memory: 3384, decode.loss_ce: 1.0466, decode.acc_seg: 31.5962, loss: 1.0466
2023-05-26 21:23:37,042 - mmseg - INFO - Iter [250/20000]	lr: 9.836e-06, eta: 2:16:41, time: 0.399, data_time: 0.153, memory: 3384, decode.loss_ce: 0.8130, decode.acc_seg: 37.0829, loss: 0.8130
2023-05-26 21:23:57,005 - mmseg - INFO - Iter [300/20000]	lr: 1.178e-05, eta: 2:15:27, time: 0.399, data_time: 0.152, memory: 3384, decode.loss_ce: 0.6893, decode.acc_seg: 33.6433, loss: 0.6893
2023-05-26 21:24:17,036 - mmseg - INFO - Iter [350/20000]	lr: 1.372e-05, eta: 2:14:33, time: 0.401, data_time: 0.152, memory: 3384, decode.loss_ce: 0.5826, decode.acc_seg: 33.5262, loss: 0.5826
2023-05-26 21:24:36,969 - mmseg - INFO - Iter [400/20000]	lr: 1.564e-05, eta: 2:13:43, time: 0.399, data_time: 0.152, memory: 3384, decode.loss_ce: 0.5590, decode.acc_seg: 38.5555, loss: 0.5590
2023-05-26 21:24:59,249 - mmseg - INFO - Iter [450/20000]	lr: 1.756e-05, eta: 2:14:41, time: 0.446, data_time: 0.201, memory: 3384, decode.loss_ce: 0.5352, decode.acc_seg: 37.4838, loss: 0.5352
2023-05-26 21:25:18,986 - mmseg - INFO - Iter [500/20000]	lr: 1.946e-05, eta: 2:13:44, time: 0.395, data_time: 0.152, memory: 3384, decode.loss_ce: 0.4871, decode.acc_seg: 39.2642, loss: 0.4871
2023-05-26 21:25:39,172 - mmseg - INFO - Iter [550/20000]	lr: 2.136e-05, eta: 2:13:09, time: 0.404, data_time: 0.153, memory: 3384, decode.loss_ce: 0.4551, decode.acc_seg: 38.6316, loss: 0.4551
2023-05-26 21:25:59,316 - mmseg - INFO - Iter [600/20000]	lr: 2.324e-05, eta: 2:12:36, time: 0.403, data_time: 0.152, memory: 3384, decode.loss_ce: 0.4589, decode.acc_seg: 38.3555, loss: 0.4589
2023-05-26 21:26:19,503 - mmseg - INFO - Iter [650/20000]	lr: 2.512e-05, eta: 2:12:06, time: 0.404, data_time: 0.157, memory: 3384, decode.loss_ce: 0.4608, decode.acc_seg: 42.9404, loss: 0.4608
2023-05-26 21:26:38,573 - mmseg - INFO - Iter [700/20000]	lr: 2.698e-05, eta: 2:11:07, time: 0.381, data_time: 0.152, memory: 3384, decode.loss_ce: 0.4103, decode.acc_seg: 43.1296, loss: 0.4103
2023-05-26 21:26:57,753 - mmseg - INFO - Iter [750/20000]	lr: 2.884e-05, eta: 2:10:15, time: 0.384, data_time: 0.153, memory: 3384, decode.loss_ce: 0.3790, decode.acc_seg: 39.8777, loss: 0.3790
2023-05-26 21:27:16,928 - mmseg - INFO - Iter [800/20000]	lr: 3.068e-05, eta: 2:09:28, time: 0.383, data_time: 0.152, memory: 3384, decode.loss_ce: 0.4028, decode.acc_seg: 42.1494, loss: 0.4028
2023-05-26 21:27:39,200 - mmseg - INFO - Iter [850/20000]	lr: 3.252e-05, eta: 2:09:54, time: 0.445, data_time: 0.201, memory: 3384, decode.loss_ce: 0.4251, decode.acc_seg: 39.3153, loss: 0.4251
2023-05-26 21:27:58,871 - mmseg - INFO - Iter [900/20000]	lr: 3.434e-05, eta: 2:09:19, time: 0.393, data_time: 0.152, memory: 3384, decode.loss_ce: 0.3657, decode.acc_seg: 43.2113, loss: 0.3657
2023-05-26 21:28:18,559 - mmseg - INFO - Iter [950/20000]	lr: 3.616e-05, eta: 2:08:46, time: 0.394, data_time: 0.153, memory: 3384, decode.loss_ce: 0.4027, decode.acc_seg: 41.7760, loss: 0.4027
2023-05-26 21:28:38,277 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 21:28:38,277 - mmseg - INFO - Iter [1000/20000]	lr: 3.796e-05, eta: 2:08:15, time: 0.394, data_time: 0.153, memory: 3384, decode.loss_ce: 0.4370, decode.acc_seg: 41.8997, loss: 0.4370
2023-05-26 21:28:58,585 - mmseg - INFO - Iter [1050/20000]	lr: 3.976e-05, eta: 2:07:56, time: 0.406, data_time: 0.153, memory: 3384, decode.loss_ce: 0.3891, decode.acc_seg: 42.1129, loss: 0.3891
2023-05-26 21:29:18,795 - mmseg - INFO - Iter [1100/20000]	lr: 4.154e-05, eta: 2:07:35, time: 0.404, data_time: 0.153, memory: 3384, decode.loss_ce: 0.4115, decode.acc_seg: 42.4891, loss: 0.4115
2023-05-26 21:29:39,153 - mmseg - INFO - Iter [1150/20000]	lr: 4.332e-05, eta: 2:07:16, time: 0.407, data_time: 0.153, memory: 3384, decode.loss_ce: 0.3849, decode.acc_seg: 42.0843, loss: 0.3849
2023-05-26 21:29:59,152 - mmseg - INFO - Iter [1200/20000]	lr: 4.508e-05, eta: 2:06:52, time: 0.400, data_time: 0.153, memory: 3384, decode.loss_ce: 0.3672, decode.acc_seg: 43.1895, loss: 0.3672
2023-05-26 21:30:21,427 - mmseg - INFO - Iter [1250/20000]	lr: 4.684e-05, eta: 2:07:02, time: 0.445, data_time: 0.201, memory: 3384, decode.loss_ce: 0.3706, decode.acc_seg: 42.5274, loss: 0.3706
2023-05-26 21:30:41,761 - mmseg - INFO - Iter [1300/20000]	lr: 4.859e-05, eta: 2:06:42, time: 0.407, data_time: 0.153, memory: 3384, decode.loss_ce: 0.3465, decode.acc_seg: 41.0766, loss: 0.3465
2023-05-26 21:31:02,117 - mmseg - INFO - Iter [1350/20000]	lr: 5.032e-05, eta: 2:06:22, time: 0.407, data_time: 0.154, memory: 3384, decode.loss_ce: 0.3477, decode.acc_seg: 43.8843, loss: 0.3477
2023-05-26 21:31:21,707 - mmseg - INFO - Iter [1400/20000]	lr: 5.205e-05, eta: 2:05:52, time: 0.392, data_time: 0.154, memory: 3384, decode.loss_ce: 0.3264, decode.acc_seg: 42.3771, loss: 0.3264
2023-05-26 21:31:41,160 - mmseg - INFO - Iter [1450/20000]	lr: 5.376e-05, eta: 2:05:21, time: 0.389, data_time: 0.153, memory: 3384, decode.loss_ce: 0.3899, decode.acc_seg: 43.4818, loss: 0.3899
2023-05-26 21:32:00,538 - mmseg - INFO - Iter [1500/20000]	lr: 5.547e-05, eta: 2:04:49, time: 0.388, data_time: 0.152, memory: 3384, decode.loss_ce: 0.3437, decode.acc_seg: 45.2539, loss: 0.3437
2023-05-26 21:32:20,144 - mmseg - INFO - Iter [1550/20000]	lr: 5.535e-05, eta: 2:04:22, time: 0.392, data_time: 0.153, memory: 3384, decode.loss_ce: 0.3777, decode.acc_seg: 39.9138, loss: 0.3777
2023-05-26 21:32:39,745 - mmseg - INFO - Iter [1600/20000]	lr: 5.520e-05, eta: 2:03:54, time: 0.392, data_time: 0.153, memory: 3384, decode.loss_ce: 0.3652, decode.acc_seg: 43.5923, loss: 0.3652
2023-05-26 21:33:01,551 - mmseg - INFO - Iter [1650/20000]	lr: 5.505e-05, eta: 2:03:52, time: 0.436, data_time: 0.201, memory: 3384, decode.loss_ce: 0.3419, decode.acc_seg: 44.9126, loss: 0.3419
2023-05-26 21:33:21,695 - mmseg - INFO - Iter [1700/20000]	lr: 5.490e-05, eta: 2:03:30, time: 0.403, data_time: 0.154, memory: 3384, decode.loss_ce: 0.3086, decode.acc_seg: 43.5135, loss: 0.3086
2023-05-26 21:33:41,742 - mmseg - INFO - Iter [1750/20000]	lr: 5.475e-05, eta: 2:03:08, time: 0.401, data_time: 0.152, memory: 3384, decode.loss_ce: 0.3773, decode.acc_seg: 44.4426, loss: 0.3773
2023-05-26 21:34:01,775 - mmseg - INFO - Iter [1800/20000]	lr: 5.460e-05, eta: 2:02:46, time: 0.401, data_time: 0.153, memory: 3384, decode.loss_ce: 0.3341, decode.acc_seg: 42.6461, loss: 0.3341
2023-05-26 21:34:22,175 - mmseg - INFO - Iter [1850/20000]	lr: 5.445e-05, eta: 2:02:27, time: 0.408, data_time: 0.155, memory: 3384, decode.loss_ce: 0.3421, decode.acc_seg: 45.6230, loss: 0.3421
2023-05-26 21:34:41,697 - mmseg - INFO - Iter [1900/20000]	lr: 5.430e-05, eta: 2:02:00, time: 0.390, data_time: 0.153, memory: 3384, decode.loss_ce: 0.2885, decode.acc_seg: 45.2789, loss: 0.2885
2023-05-26 21:35:01,767 - mmseg - INFO - Iter [1950/20000]	lr: 5.415e-05, eta: 2:01:38, time: 0.401, data_time: 0.152, memory: 3384, decode.loss_ce: 0.4047, decode.acc_seg: 40.7358, loss: 0.4047
2023-05-26 21:35:21,698 - mmseg - INFO - Saving checkpoint at 2000 iterations
2023-05-26 21:35:24,123 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 21:35:24,123 - mmseg - INFO - Iter [2000/20000]	lr: 5.400e-05, eta: 2:01:37, time: 0.447, data_time: 0.152, memory: 3384, decode.loss_ce: 0.3218, decode.acc_seg: 43.1120, loss: 0.3218
2023-05-26 21:35:26,797 - mmseg - INFO - per class results:
2023-05-26 21:35:26,800 - mmseg - INFO - 
+---------------+-------+-------+
|     Class     |  IoU  |  Acc  |
+---------------+-------+-------+
|   Dry Grass   |  0.0  |  0.0  |
|  Green Grass  | 83.29 | 96.87 |
|   Dry Shrubs  |  0.0  |  0.0  |
|  Green Shrubs |  0.0  |  0.0  |
|     Canopy    |  0.0  |  0.0  |
|  Wood Pieces  | 86.93 | 98.93 |
|   Litterfall  |  nan  |  nan  |
| Timber Litter |  0.0  |  0.0  |
|  Live Trunks  |  0.0  |  0.0  |
|   Bare Earth  |  0.18 |  0.48 |
|     People    | 92.73 |  93.5 |
|      Sky      |  nan  |  nan  |
|     Blurry    |  nan  |  nan  |
|   Obstacles   |  nan  |  nan  |
+---------------+-------+-------+
2023-05-26 21:35:26,800 - mmseg - INFO - Summary:
2023-05-26 21:35:26,800 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 90.32 | 26.31 | 28.98 |
+-------+-------+-------+
2023-05-26 21:35:26,801 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 21:35:26,802 - mmseg - INFO - Iter(val) [11]	aAcc: 0.9032, mIoU: 0.2631, mAcc: 0.2898, IoU.Dry Grass: 0.0000, IoU.Green Grass: 0.8329, IoU.Dry Shrubs: 0.0000, IoU.Green Shrubs: 0.0000, IoU.Canopy: 0.0000, IoU.Wood Pieces: 0.8693, IoU.Litterfall: nan, IoU.Timber Litter: 0.0000, IoU.Live Trunks: 0.0000, IoU.Bare Earth: 0.0018, IoU.People: 0.9273, IoU.Sky: nan, IoU.Blurry: nan, IoU.Obstacles: nan, Acc.Dry Grass: 0.0000, Acc.Green Grass: 0.9687, Acc.Dry Shrubs: 0.0000, Acc.Green Shrubs: 0.0000, Acc.Canopy: 0.0000, Acc.Wood Pieces: 0.9893, Acc.Litterfall: nan, Acc.Timber Litter: 0.0000, Acc.Live Trunks: 0.0000, Acc.Bare Earth: 0.0048, Acc.People: 0.9350, Acc.Sky: nan, Acc.Blurry: nan, Acc.Obstacles: nan
2023-05-26 21:35:48,877 - mmseg - INFO - Iter [2050/20000]	lr: 5.385e-05, eta: 2:01:56, time: 0.495, data_time: 0.255, memory: 3387, decode.loss_ce: 0.2744, decode.acc_seg: 44.3637, loss: 0.2744
2023-05-26 21:36:09,142 - mmseg - INFO - Iter [2100/20000]	lr: 5.370e-05, eta: 2:01:35, time: 0.405, data_time: 0.153, memory: 3387, decode.loss_ce: 0.3319, decode.acc_seg: 45.2712, loss: 0.3319
2023-05-26 21:36:29,419 - mmseg - INFO - Iter [2150/20000]	lr: 5.355e-05, eta: 2:01:14, time: 0.406, data_time: 0.156, memory: 3387, decode.loss_ce: 0.3109, decode.acc_seg: 45.0938, loss: 0.3109
2023-05-26 21:36:49,646 - mmseg - INFO - Iter [2200/20000]	lr: 5.340e-05, eta: 2:00:52, time: 0.405, data_time: 0.155, memory: 3387, decode.loss_ce: 0.3071, decode.acc_seg: 44.2363, loss: 0.3071
2023-05-26 21:37:09,710 - mmseg - INFO - Iter [2250/20000]	lr: 5.325e-05, eta: 2:00:29, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.2835, decode.acc_seg: 47.1221, loss: 0.2835
2023-05-26 21:37:29,762 - mmseg - INFO - Iter [2300/20000]	lr: 5.310e-05, eta: 2:00:07, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.2496, decode.acc_seg: 44.6419, loss: 0.2496
2023-05-26 21:37:49,848 - mmseg - INFO - Iter [2350/20000]	lr: 5.295e-05, eta: 1:59:44, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.3074, decode.acc_seg: 44.3391, loss: 0.3074
2023-05-26 21:38:10,033 - mmseg - INFO - Iter [2400/20000]	lr: 5.280e-05, eta: 1:59:23, time: 0.404, data_time: 0.154, memory: 3387, decode.loss_ce: 0.2877, decode.acc_seg: 43.2374, loss: 0.2877
2023-05-26 21:38:32,567 - mmseg - INFO - Iter [2450/20000]	lr: 5.265e-05, eta: 1:59:18, time: 0.451, data_time: 0.201, memory: 3387, decode.loss_ce: 0.3119, decode.acc_seg: 44.0355, loss: 0.3119
2023-05-26 21:38:52,709 - mmseg - INFO - Iter [2500/20000]	lr: 5.250e-05, eta: 1:58:56, time: 0.403, data_time: 0.153, memory: 3387, decode.loss_ce: 0.2771, decode.acc_seg: 46.4435, loss: 0.2771
2023-05-26 21:39:12,805 - mmseg - INFO - Iter [2550/20000]	lr: 5.235e-05, eta: 1:58:33, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.2870, decode.acc_seg: 44.0088, loss: 0.2870
2023-05-26 21:39:32,918 - mmseg - INFO - Iter [2600/20000]	lr: 5.220e-05, eta: 1:58:11, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.2676, decode.acc_seg: 46.8900, loss: 0.2676
2023-05-26 21:39:53,037 - mmseg - INFO - Iter [2650/20000]	lr: 5.205e-05, eta: 1:57:49, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.3405, decode.acc_seg: 42.2621, loss: 0.3405
2023-05-26 21:40:13,181 - mmseg - INFO - Iter [2700/20000]	lr: 5.190e-05, eta: 1:57:27, time: 0.403, data_time: 0.152, memory: 3387, decode.loss_ce: 0.2640, decode.acc_seg: 45.9487, loss: 0.2640
2023-05-26 21:40:33,447 - mmseg - INFO - Iter [2750/20000]	lr: 5.175e-05, eta: 1:57:06, time: 0.405, data_time: 0.153, memory: 3387, decode.loss_ce: 0.2736, decode.acc_seg: 43.5731, loss: 0.2736
2023-05-26 21:40:53,614 - mmseg - INFO - Iter [2800/20000]	lr: 5.160e-05, eta: 1:56:45, time: 0.403, data_time: 0.153, memory: 3387, decode.loss_ce: 0.2985, decode.acc_seg: 45.1195, loss: 0.2985
2023-05-26 21:41:16,232 - mmseg - INFO - Iter [2850/20000]	lr: 5.145e-05, eta: 1:56:38, time: 0.452, data_time: 0.201, memory: 3387, decode.loss_ce: 0.2762, decode.acc_seg: 45.4555, loss: 0.2762
2023-05-26 21:41:36,359 - mmseg - INFO - Iter [2900/20000]	lr: 5.130e-05, eta: 1:56:16, time: 0.403, data_time: 0.152, memory: 3387, decode.loss_ce: 0.2130, decode.acc_seg: 46.1143, loss: 0.2130
2023-05-26 21:41:56,437 - mmseg - INFO - Iter [2950/20000]	lr: 5.115e-05, eta: 1:55:53, time: 0.402, data_time: 0.153, memory: 3387, decode.loss_ce: 0.2563, decode.acc_seg: 47.7583, loss: 0.2563
2023-05-26 21:42:15,680 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 21:42:15,680 - mmseg - INFO - Iter [3000/20000]	lr: 5.100e-05, eta: 1:55:27, time: 0.385, data_time: 0.153, memory: 3387, decode.loss_ce: 0.2579, decode.acc_seg: 44.4423, loss: 0.2579
2023-05-26 21:42:34,871 - mmseg - INFO - Iter [3050/20000]	lr: 5.085e-05, eta: 1:55:00, time: 0.384, data_time: 0.153, memory: 3387, decode.loss_ce: 0.2290, decode.acc_seg: 44.5783, loss: 0.2290
2023-05-26 21:42:54,437 - mmseg - INFO - Iter [3100/20000]	lr: 5.070e-05, eta: 1:54:35, time: 0.391, data_time: 0.153, memory: 3387, decode.loss_ce: 0.2537, decode.acc_seg: 46.5937, loss: 0.2537
2023-05-26 21:43:14,384 - mmseg - INFO - Iter [3150/20000]	lr: 5.055e-05, eta: 1:54:13, time: 0.399, data_time: 0.152, memory: 3387, decode.loss_ce: 0.2142, decode.acc_seg: 46.4738, loss: 0.2142
2023-05-26 21:43:34,611 - mmseg - INFO - Iter [3200/20000]	lr: 5.040e-05, eta: 1:53:52, time: 0.405, data_time: 0.152, memory: 3387, decode.loss_ce: 0.2736, decode.acc_seg: 44.3639, loss: 0.2736
2023-05-26 21:43:57,204 - mmseg - INFO - Iter [3250/20000]	lr: 5.025e-05, eta: 1:53:43, time: 0.452, data_time: 0.201, memory: 3387, decode.loss_ce: 0.2144, decode.acc_seg: 47.2817, loss: 0.2144
2023-05-26 21:44:17,478 - mmseg - INFO - Iter [3300/20000]	lr: 5.010e-05, eta: 1:53:22, time: 0.405, data_time: 0.153, memory: 3387, decode.loss_ce: 0.2051, decode.acc_seg: 48.5086, loss: 0.2051
2023-05-26 21:44:37,619 - mmseg - INFO - Iter [3350/20000]	lr: 4.995e-05, eta: 1:53:01, time: 0.403, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1836, decode.acc_seg: 46.3089, loss: 0.1836
2023-05-26 21:44:57,847 - mmseg - INFO - Iter [3400/20000]	lr: 4.980e-05, eta: 1:52:40, time: 0.405, data_time: 0.152, memory: 3387, decode.loss_ce: 0.2001, decode.acc_seg: 45.8676, loss: 0.2001
2023-05-26 21:45:17,085 - mmseg - INFO - Iter [3450/20000]	lr: 4.965e-05, eta: 1:52:14, time: 0.385, data_time: 0.152, memory: 3387, decode.loss_ce: 0.2386, decode.acc_seg: 48.1164, loss: 0.2386
2023-05-26 21:45:36,348 - mmseg - INFO - Iter [3500/20000]	lr: 4.950e-05, eta: 1:51:48, time: 0.385, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1879, decode.acc_seg: 42.4731, loss: 0.1879
2023-05-26 21:45:56,514 - mmseg - INFO - Iter [3550/20000]	lr: 4.935e-05, eta: 1:51:27, time: 0.403, data_time: 0.152, memory: 3387, decode.loss_ce: 0.2110, decode.acc_seg: 50.1084, loss: 0.2110
2023-05-26 21:46:16,741 - mmseg - INFO - Iter [3600/20000]	lr: 4.920e-05, eta: 1:51:06, time: 0.405, data_time: 0.153, memory: 3387, decode.loss_ce: 0.2060, decode.acc_seg: 45.9530, loss: 0.2060
2023-05-26 21:46:39,672 - mmseg - INFO - Iter [3650/20000]	lr: 4.905e-05, eta: 1:50:58, time: 0.459, data_time: 0.203, memory: 3387, decode.loss_ce: 0.3466, decode.acc_seg: 43.3717, loss: 0.3466
2023-05-26 21:46:59,798 - mmseg - INFO - Iter [3700/20000]	lr: 4.890e-05, eta: 1:50:36, time: 0.403, data_time: 0.152, memory: 3387, decode.loss_ce: 0.2592, decode.acc_seg: 47.7630, loss: 0.2592
2023-05-26 21:47:20,034 - mmseg - INFO - Iter [3750/20000]	lr: 4.875e-05, eta: 1:50:16, time: 0.405, data_time: 0.153, memory: 3387, decode.loss_ce: 0.2234, decode.acc_seg: 47.3016, loss: 0.2234
2023-05-26 21:47:40,363 - mmseg - INFO - Iter [3800/20000]	lr: 4.860e-05, eta: 1:49:55, time: 0.407, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1938, decode.acc_seg: 48.3171, loss: 0.1938
2023-05-26 21:48:00,703 - mmseg - INFO - Iter [3850/20000]	lr: 4.845e-05, eta: 1:49:35, time: 0.407, data_time: 0.154, memory: 3387, decode.loss_ce: 0.2125, decode.acc_seg: 46.0561, loss: 0.2125
2023-05-26 21:48:20,307 - mmseg - INFO - Iter [3900/20000]	lr: 4.830e-05, eta: 1:49:11, time: 0.392, data_time: 0.152, memory: 3387, decode.loss_ce: 0.2121, decode.acc_seg: 45.4999, loss: 0.2121
2023-05-26 21:48:40,191 - mmseg - INFO - Iter [3950/20000]	lr: 4.815e-05, eta: 1:48:49, time: 0.398, data_time: 0.152, memory: 3387, decode.loss_ce: 0.2489, decode.acc_seg: 46.0894, loss: 0.2489
2023-05-26 21:48:59,390 - mmseg - INFO - Saving checkpoint at 4000 iterations
2023-05-26 21:49:02,434 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 21:49:02,434 - mmseg - INFO - Iter [4000/20000]	lr: 4.800e-05, eta: 1:48:36, time: 0.445, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1918, decode.acc_seg: 48.7129, loss: 0.1918
2023-05-26 21:49:04,950 - mmseg - INFO - per class results:
2023-05-26 21:49:04,951 - mmseg - INFO - 
+---------------+-------+-------+
|     Class     |  IoU  |  Acc  |
+---------------+-------+-------+
|   Dry Grass   |  0.0  |  0.0  |
|  Green Grass  | 82.15 | 98.53 |
|   Dry Shrubs  |  0.19 |  0.19 |
|  Green Shrubs |  0.0  |  0.0  |
|     Canopy    |  2.36 |  2.54 |
|  Wood Pieces  | 90.35 | 97.76 |
|   Litterfall  |  nan  |  nan  |
| Timber Litter |  0.0  |  0.0  |
|  Live Trunks  |  0.0  |  0.0  |
|   Bare Earth  |  6.88 |  6.89 |
|     People    | 92.59 | 93.43 |
|      Sky      |  nan  |  nan  |
|     Blurry    |  nan  |  nan  |
|   Obstacles   |  nan  |  nan  |
+---------------+-------+-------+
2023-05-26 21:49:04,951 - mmseg - INFO - Summary:
2023-05-26 21:49:04,952 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 90.56 | 27.45 | 29.93 |
+-------+-------+-------+
2023-05-26 21:49:04,952 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 21:49:04,952 - mmseg - INFO - Iter(val) [11]	aAcc: 0.9056, mIoU: 0.2745, mAcc: 0.2993, IoU.Dry Grass: 0.0000, IoU.Green Grass: 0.8215, IoU.Dry Shrubs: 0.0019, IoU.Green Shrubs: 0.0000, IoU.Canopy: 0.0236, IoU.Wood Pieces: 0.9035, IoU.Litterfall: nan, IoU.Timber Litter: 0.0000, IoU.Live Trunks: 0.0000, IoU.Bare Earth: 0.0688, IoU.People: 0.9259, IoU.Sky: nan, IoU.Blurry: nan, IoU.Obstacles: nan, Acc.Dry Grass: 0.0000, Acc.Green Grass: 0.9853, Acc.Dry Shrubs: 0.0019, Acc.Green Shrubs: 0.0000, Acc.Canopy: 0.0254, Acc.Wood Pieces: 0.9776, Acc.Litterfall: nan, Acc.Timber Litter: 0.0000, Acc.Live Trunks: 0.0000, Acc.Bare Earth: 0.0689, Acc.People: 0.9343, Acc.Sky: nan, Acc.Blurry: nan, Acc.Obstacles: nan
2023-05-26 21:49:27,433 - mmseg - INFO - Iter [4050/20000]	lr: 4.785e-05, eta: 1:48:34, time: 0.500, data_time: 0.252, memory: 3387, decode.loss_ce: 0.2288, decode.acc_seg: 47.0835, loss: 0.2288
2023-05-26 21:49:47,166 - mmseg - INFO - Iter [4100/20000]	lr: 4.770e-05, eta: 1:48:11, time: 0.395, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1932, decode.acc_seg: 46.5380, loss: 0.1932
2023-05-26 21:50:07,402 - mmseg - INFO - Iter [4150/20000]	lr: 4.755e-05, eta: 1:47:50, time: 0.405, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1605, decode.acc_seg: 45.9593, loss: 0.1605
2023-05-26 21:50:27,704 - mmseg - INFO - Iter [4200/20000]	lr: 4.740e-05, eta: 1:47:29, time: 0.406, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1888, decode.acc_seg: 49.0518, loss: 0.1888
2023-05-26 21:50:48,073 - mmseg - INFO - Iter [4250/20000]	lr: 4.725e-05, eta: 1:47:08, time: 0.407, data_time: 0.152, memory: 3387, decode.loss_ce: 0.2166, decode.acc_seg: 45.4619, loss: 0.2166
2023-05-26 21:51:08,248 - mmseg - INFO - Iter [4300/20000]	lr: 4.710e-05, eta: 1:46:47, time: 0.403, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1855, decode.acc_seg: 45.3955, loss: 0.1855
2023-05-26 21:51:28,594 - mmseg - INFO - Iter [4350/20000]	lr: 4.695e-05, eta: 1:46:27, time: 0.407, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1986, decode.acc_seg: 49.5547, loss: 0.1986
2023-05-26 21:51:48,817 - mmseg - INFO - Iter [4400/20000]	lr: 4.680e-05, eta: 1:46:06, time: 0.404, data_time: 0.152, memory: 3387, decode.loss_ce: 0.2335, decode.acc_seg: 46.2637, loss: 0.2335
2023-05-26 21:52:11,531 - mmseg - INFO - Iter [4450/20000]	lr: 4.665e-05, eta: 1:45:53, time: 0.454, data_time: 0.201, memory: 3387, decode.loss_ce: 0.1986, decode.acc_seg: 48.6918, loss: 0.1986
2023-05-26 21:52:31,749 - mmseg - INFO - Iter [4500/20000]	lr: 4.650e-05, eta: 1:45:32, time: 0.404, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1956, decode.acc_seg: 45.7415, loss: 0.1956
2023-05-26 21:52:51,986 - mmseg - INFO - Iter [4550/20000]	lr: 4.635e-05, eta: 1:45:11, time: 0.405, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1839, decode.acc_seg: 47.8239, loss: 0.1839
2023-05-26 21:53:12,241 - mmseg - INFO - Iter [4600/20000]	lr: 4.620e-05, eta: 1:44:50, time: 0.405, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1672, decode.acc_seg: 48.3212, loss: 0.1672
2023-05-26 21:53:32,489 - mmseg - INFO - Iter [4650/20000]	lr: 4.605e-05, eta: 1:44:29, time: 0.405, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1802, decode.acc_seg: 48.3703, loss: 0.1802
2023-05-26 21:53:52,589 - mmseg - INFO - Iter [4700/20000]	lr: 4.590e-05, eta: 1:44:07, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.2157, decode.acc_seg: 46.3602, loss: 0.2157
2023-05-26 21:54:12,706 - mmseg - INFO - Iter [4750/20000]	lr: 4.575e-05, eta: 1:43:46, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.2206, decode.acc_seg: 46.3600, loss: 0.2206
2023-05-26 21:54:32,958 - mmseg - INFO - Iter [4800/20000]	lr: 4.560e-05, eta: 1:43:25, time: 0.405, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1447, decode.acc_seg: 47.0610, loss: 0.1447
2023-05-26 21:54:55,585 - mmseg - INFO - Iter [4850/20000]	lr: 4.545e-05, eta: 1:43:12, time: 0.453, data_time: 0.201, memory: 3387, decode.loss_ce: 0.2211, decode.acc_seg: 48.5593, loss: 0.2211
2023-05-26 21:55:15,957 - mmseg - INFO - Iter [4900/20000]	lr: 4.530e-05, eta: 1:42:51, time: 0.407, data_time: 0.156, memory: 3387, decode.loss_ce: 0.2688, decode.acc_seg: 45.9803, loss: 0.2688
2023-05-26 21:55:36,121 - mmseg - INFO - Iter [4950/20000]	lr: 4.515e-05, eta: 1:42:30, time: 0.403, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1898, decode.acc_seg: 43.9657, loss: 0.1898
2023-05-26 21:55:56,394 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 21:55:56,395 - mmseg - INFO - Iter [5000/20000]	lr: 4.500e-05, eta: 1:42:09, time: 0.405, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1780, decode.acc_seg: 49.7075, loss: 0.1780
2023-05-26 21:56:16,695 - mmseg - INFO - Iter [5050/20000]	lr: 4.485e-05, eta: 1:41:48, time: 0.406, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1958, decode.acc_seg: 50.5284, loss: 0.1958
2023-05-26 21:56:36,732 - mmseg - INFO - Iter [5100/20000]	lr: 4.470e-05, eta: 1:41:26, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1516, decode.acc_seg: 49.4516, loss: 0.1516
2023-05-26 21:56:56,794 - mmseg - INFO - Iter [5150/20000]	lr: 4.455e-05, eta: 1:41:05, time: 0.401, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1461, decode.acc_seg: 50.4392, loss: 0.1461
2023-05-26 21:57:16,842 - mmseg - INFO - Iter [5200/20000]	lr: 4.440e-05, eta: 1:40:43, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1536, decode.acc_seg: 50.0790, loss: 0.1536
2023-05-26 21:57:39,304 - mmseg - INFO - Iter [5250/20000]	lr: 4.425e-05, eta: 1:40:29, time: 0.449, data_time: 0.201, memory: 3387, decode.loss_ce: 0.1399, decode.acc_seg: 47.9937, loss: 0.1399
2023-05-26 21:57:59,392 - mmseg - INFO - Iter [5300/20000]	lr: 4.410e-05, eta: 1:40:07, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1724, decode.acc_seg: 51.1003, loss: 0.1724
2023-05-26 21:58:19,874 - mmseg - INFO - Iter [5350/20000]	lr: 4.395e-05, eta: 1:39:47, time: 0.410, data_time: 0.155, memory: 3387, decode.loss_ce: 0.1585, decode.acc_seg: 49.4260, loss: 0.1585
2023-05-26 21:58:40,079 - mmseg - INFO - Iter [5400/20000]	lr: 4.380e-05, eta: 1:39:26, time: 0.404, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1556, decode.acc_seg: 49.5155, loss: 0.1556
2023-05-26 21:59:00,115 - mmseg - INFO - Iter [5450/20000]	lr: 4.365e-05, eta: 1:39:05, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1525, decode.acc_seg: 47.0093, loss: 0.1525
2023-05-26 21:59:20,277 - mmseg - INFO - Iter [5500/20000]	lr: 4.350e-05, eta: 1:38:43, time: 0.403, data_time: 0.155, memory: 3387, decode.loss_ce: 0.1368, decode.acc_seg: 51.4083, loss: 0.1368
2023-05-26 21:59:40,361 - mmseg - INFO - Iter [5550/20000]	lr: 4.335e-05, eta: 1:38:22, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1519, decode.acc_seg: 49.1448, loss: 0.1519
2023-05-26 22:00:00,428 - mmseg - INFO - Iter [5600/20000]	lr: 4.320e-05, eta: 1:38:01, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1441, decode.acc_seg: 48.1207, loss: 0.1441
2023-05-26 22:00:23,033 - mmseg - INFO - Iter [5650/20000]	lr: 4.305e-05, eta: 1:37:46, time: 0.452, data_time: 0.201, memory: 3387, decode.loss_ce: 0.1363, decode.acc_seg: 49.7040, loss: 0.1363
2023-05-26 22:00:43,201 - mmseg - INFO - Iter [5700/20000]	lr: 4.290e-05, eta: 1:37:25, time: 0.403, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1648, decode.acc_seg: 48.0831, loss: 0.1648
2023-05-26 22:01:03,568 - mmseg - INFO - Iter [5750/20000]	lr: 4.275e-05, eta: 1:37:04, time: 0.407, data_time: 0.155, memory: 3387, decode.loss_ce: 0.1459, decode.acc_seg: 47.4608, loss: 0.1459
2023-05-26 22:01:23,805 - mmseg - INFO - Iter [5800/20000]	lr: 4.260e-05, eta: 1:36:43, time: 0.405, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1410, decode.acc_seg: 46.6810, loss: 0.1410
2023-05-26 22:01:43,924 - mmseg - INFO - Iter [5850/20000]	lr: 4.245e-05, eta: 1:36:22, time: 0.402, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1475, decode.acc_seg: 46.6880, loss: 0.1475
2023-05-26 22:02:04,064 - mmseg - INFO - Iter [5900/20000]	lr: 4.230e-05, eta: 1:36:01, time: 0.403, data_time: 0.153, memory: 3387, decode.loss_ce: 0.2016, decode.acc_seg: 47.3873, loss: 0.2016
2023-05-26 22:02:24,540 - mmseg - INFO - Iter [5950/20000]	lr: 4.215e-05, eta: 1:35:41, time: 0.410, data_time: 0.154, memory: 3387, decode.loss_ce: 0.1756, decode.acc_seg: 50.9715, loss: 0.1756
2023-05-26 22:02:44,794 - mmseg - INFO - Saving checkpoint at 6000 iterations
2023-05-26 22:02:47,966 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 22:02:47,967 - mmseg - INFO - Iter [6000/20000]	lr: 4.200e-05, eta: 1:35:27, time: 0.469, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1311, decode.acc_seg: 48.1838, loss: 0.1311
2023-05-26 22:02:50,316 - mmseg - INFO - per class results:
2023-05-26 22:02:50,317 - mmseg - INFO - 
+---------------+-------+-------+
|     Class     |  IoU  |  Acc  |
+---------------+-------+-------+
|   Dry Grass   |  0.0  |  0.0  |
|  Green Grass  | 88.22 |  97.1 |
|   Dry Shrubs  | 43.02 | 44.92 |
|  Green Shrubs |  0.0  |  0.0  |
|     Canopy    | 25.73 | 31.81 |
|  Wood Pieces  | 93.36 | 99.07 |
|   Litterfall  |  nan  |  nan  |
| Timber Litter |  0.0  |  0.0  |
|  Live Trunks  |  0.0  |  0.0  |
|   Bare Earth  | 69.65 | 70.79 |
|     People    | 94.95 | 96.25 |
|      Sky      |  nan  |  nan  |
|     Blurry    |  nan  |  nan  |
|   Obstacles   |  nan  |  nan  |
+---------------+-------+-------+
2023-05-26 22:02:50,318 - mmseg - INFO - Summary:
2023-05-26 22:02:50,318 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 93.69 | 41.49 | 43.99 |
+-------+-------+-------+
2023-05-26 22:02:50,319 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 22:02:50,319 - mmseg - INFO - Iter(val) [11]	aAcc: 0.9369, mIoU: 0.4149, mAcc: 0.4399, IoU.Dry Grass: 0.0000, IoU.Green Grass: 0.8822, IoU.Dry Shrubs: 0.4302, IoU.Green Shrubs: 0.0000, IoU.Canopy: 0.2573, IoU.Wood Pieces: 0.9336, IoU.Litterfall: nan, IoU.Timber Litter: 0.0000, IoU.Live Trunks: 0.0000, IoU.Bare Earth: 0.6965, IoU.People: 0.9495, IoU.Sky: nan, IoU.Blurry: nan, IoU.Obstacles: nan, Acc.Dry Grass: 0.0000, Acc.Green Grass: 0.9710, Acc.Dry Shrubs: 0.4492, Acc.Green Shrubs: 0.0000, Acc.Canopy: 0.3181, Acc.Wood Pieces: 0.9907, Acc.Litterfall: nan, Acc.Timber Litter: 0.0000, Acc.Live Trunks: 0.0000, Acc.Bare Earth: 0.7079, Acc.People: 0.9625, Acc.Sky: nan, Acc.Blurry: nan, Acc.Obstacles: nan
2023-05-26 22:03:12,405 - mmseg - INFO - Iter [6050/20000]	lr: 4.185e-05, eta: 1:35:16, time: 0.489, data_time: 0.250, memory: 3387, decode.loss_ce: 0.1491, decode.acc_seg: 48.5493, loss: 0.1491
2023-05-26 22:03:32,173 - mmseg - INFO - Iter [6100/20000]	lr: 4.170e-05, eta: 1:34:54, time: 0.395, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1470, decode.acc_seg: 48.3366, loss: 0.1470
2023-05-26 22:03:52,284 - mmseg - INFO - Iter [6150/20000]	lr: 4.155e-05, eta: 1:34:32, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1485, decode.acc_seg: 51.4238, loss: 0.1485
2023-05-26 22:04:12,444 - mmseg - INFO - Iter [6200/20000]	lr: 4.140e-05, eta: 1:34:11, time: 0.403, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1538, decode.acc_seg: 46.4906, loss: 0.1538
2023-05-26 22:04:32,638 - mmseg - INFO - Iter [6250/20000]	lr: 4.125e-05, eta: 1:33:50, time: 0.404, data_time: 0.154, memory: 3387, decode.loss_ce: 0.1274, decode.acc_seg: 49.8663, loss: 0.1274
2023-05-26 22:04:52,744 - mmseg - INFO - Iter [6300/20000]	lr: 4.110e-05, eta: 1:33:29, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1499, decode.acc_seg: 47.9063, loss: 0.1499
2023-05-26 22:05:12,832 - mmseg - INFO - Iter [6350/20000]	lr: 4.095e-05, eta: 1:33:07, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1557, decode.acc_seg: 50.7219, loss: 0.1557
2023-05-26 22:05:32,951 - mmseg - INFO - Iter [6400/20000]	lr: 4.080e-05, eta: 1:32:46, time: 0.402, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1458, decode.acc_seg: 49.5553, loss: 0.1458
2023-05-26 22:05:55,650 - mmseg - INFO - Iter [6450/20000]	lr: 4.065e-05, eta: 1:32:31, time: 0.454, data_time: 0.202, memory: 3387, decode.loss_ce: 0.1285, decode.acc_seg: 49.6474, loss: 0.1285
2023-05-26 22:06:15,820 - mmseg - INFO - Iter [6500/20000]	lr: 4.050e-05, eta: 1:32:09, time: 0.403, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1762, decode.acc_seg: 49.0651, loss: 0.1762
2023-05-26 22:06:35,850 - mmseg - INFO - Iter [6550/20000]	lr: 4.035e-05, eta: 1:31:48, time: 0.401, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1377, decode.acc_seg: 48.7395, loss: 0.1377
2023-05-26 22:06:55,948 - mmseg - INFO - Iter [6600/20000]	lr: 4.020e-05, eta: 1:31:27, time: 0.402, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1289, decode.acc_seg: 51.7487, loss: 0.1289
2023-05-26 22:07:16,015 - mmseg - INFO - Iter [6650/20000]	lr: 4.005e-05, eta: 1:31:05, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1239, decode.acc_seg: 47.8624, loss: 0.1239
2023-05-26 22:07:36,059 - mmseg - INFO - Iter [6700/20000]	lr: 3.990e-05, eta: 1:30:44, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1429, decode.acc_seg: 50.1360, loss: 0.1429
2023-05-26 22:07:56,244 - mmseg - INFO - Iter [6750/20000]	lr: 3.975e-05, eta: 1:30:23, time: 0.404, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1173, decode.acc_seg: 51.5457, loss: 0.1173
2023-05-26 22:08:16,393 - mmseg - INFO - Iter [6800/20000]	lr: 3.960e-05, eta: 1:30:02, time: 0.403, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1741, decode.acc_seg: 48.8752, loss: 0.1741
2023-05-26 22:08:38,976 - mmseg - INFO - Iter [6850/20000]	lr: 3.945e-05, eta: 1:29:46, time: 0.452, data_time: 0.202, memory: 3387, decode.loss_ce: 0.1531, decode.acc_seg: 48.0867, loss: 0.1531
2023-05-26 22:08:59,100 - mmseg - INFO - Iter [6900/20000]	lr: 3.930e-05, eta: 1:29:24, time: 0.402, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1353, decode.acc_seg: 48.1007, loss: 0.1353
2023-05-26 22:09:19,206 - mmseg - INFO - Iter [6950/20000]	lr: 3.915e-05, eta: 1:29:03, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1230, decode.acc_seg: 50.7694, loss: 0.1230
2023-05-26 22:09:39,334 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 22:09:39,334 - mmseg - INFO - Iter [7000/20000]	lr: 3.900e-05, eta: 1:28:42, time: 0.403, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1141, decode.acc_seg: 48.7473, loss: 0.1141
2023-05-26 22:09:58,904 - mmseg - INFO - Iter [7050/20000]	lr: 3.885e-05, eta: 1:28:20, time: 0.391, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1893, decode.acc_seg: 45.5161, loss: 0.1893
2023-05-26 22:10:18,052 - mmseg - INFO - Iter [7100/20000]	lr: 3.870e-05, eta: 1:27:57, time: 0.383, data_time: 0.152, memory: 3387, decode.loss_ce: 0.2423, decode.acc_seg: 49.3528, loss: 0.2423
2023-05-26 22:10:37,594 - mmseg - INFO - Iter [7150/20000]	lr: 3.855e-05, eta: 1:27:35, time: 0.391, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1878, decode.acc_seg: 48.6253, loss: 0.1878
2023-05-26 22:10:57,877 - mmseg - INFO - Iter [7200/20000]	lr: 3.840e-05, eta: 1:27:14, time: 0.406, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1784, decode.acc_seg: 47.1913, loss: 0.1784
2023-05-26 22:11:20,420 - mmseg - INFO - Iter [7250/20000]	lr: 3.825e-05, eta: 1:26:58, time: 0.451, data_time: 0.201, memory: 3387, decode.loss_ce: 0.1617, decode.acc_seg: 49.5133, loss: 0.1617
2023-05-26 22:11:40,497 - mmseg - INFO - Iter [7300/20000]	lr: 3.810e-05, eta: 1:26:36, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1228, decode.acc_seg: 48.9335, loss: 0.1228
2023-05-26 22:12:00,669 - mmseg - INFO - Iter [7350/20000]	lr: 3.795e-05, eta: 1:26:15, time: 0.403, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1402, decode.acc_seg: 49.7453, loss: 0.1402
2023-05-26 22:12:20,787 - mmseg - INFO - Iter [7400/20000]	lr: 3.780e-05, eta: 1:25:54, time: 0.402, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1263, decode.acc_seg: 49.5016, loss: 0.1263
2023-05-26 22:12:41,104 - mmseg - INFO - Iter [7450/20000]	lr: 3.765e-05, eta: 1:25:34, time: 0.406, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1180, decode.acc_seg: 49.8521, loss: 0.1180
2023-05-26 22:13:01,266 - mmseg - INFO - Iter [7500/20000]	lr: 3.750e-05, eta: 1:25:13, time: 0.403, data_time: 0.153, memory: 3387, decode.loss_ce: 0.2015, decode.acc_seg: 45.6503, loss: 0.2015
2023-05-26 22:13:21,351 - mmseg - INFO - Iter [7550/20000]	lr: 3.735e-05, eta: 1:24:52, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.3269, decode.acc_seg: 42.6635, loss: 0.3269
2023-05-26 22:13:41,382 - mmseg - INFO - Iter [7600/20000]	lr: 3.720e-05, eta: 1:24:31, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.2236, decode.acc_seg: 44.9557, loss: 0.2236
2023-05-26 22:14:04,040 - mmseg - INFO - Iter [7650/20000]	lr: 3.705e-05, eta: 1:24:14, time: 0.453, data_time: 0.203, memory: 3387, decode.loss_ce: 0.1368, decode.acc_seg: 47.9396, loss: 0.1368
2023-05-26 22:14:24,045 - mmseg - INFO - Iter [7700/20000]	lr: 3.690e-05, eta: 1:23:53, time: 0.400, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1473, decode.acc_seg: 49.9576, loss: 0.1473
2023-05-26 22:14:44,232 - mmseg - INFO - Iter [7750/20000]	lr: 3.675e-05, eta: 1:23:32, time: 0.404, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1539, decode.acc_seg: 49.3613, loss: 0.1539
2023-05-26 22:15:04,438 - mmseg - INFO - Iter [7800/20000]	lr: 3.660e-05, eta: 1:23:11, time: 0.404, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1331, decode.acc_seg: 47.9359, loss: 0.1331
2023-05-26 22:15:24,546 - mmseg - INFO - Iter [7850/20000]	lr: 3.645e-05, eta: 1:22:50, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1307, decode.acc_seg: 44.5044, loss: 0.1307
2023-05-26 22:15:44,628 - mmseg - INFO - Iter [7900/20000]	lr: 3.630e-05, eta: 1:22:29, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1231, decode.acc_seg: 50.4321, loss: 0.1231
2023-05-26 22:16:04,649 - mmseg - INFO - Iter [7950/20000]	lr: 3.615e-05, eta: 1:22:08, time: 0.400, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1216, decode.acc_seg: 49.2549, loss: 0.1216
2023-05-26 22:16:24,966 - mmseg - INFO - Saving checkpoint at 8000 iterations
2023-05-26 22:16:26,908 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 22:16:26,909 - mmseg - INFO - Iter [8000/20000]	lr: 3.600e-05, eta: 1:21:50, time: 0.445, data_time: 0.154, memory: 3387, decode.loss_ce: 0.1407, decode.acc_seg: 48.8671, loss: 0.1407
2023-05-26 22:16:29,726 - mmseg - INFO - per class results:
2023-05-26 22:16:29,727 - mmseg - INFO - 
+---------------+-------+-------+
|     Class     |  IoU  |  Acc  |
+---------------+-------+-------+
|   Dry Grass   |  0.0  |  0.0  |
|  Green Grass  | 87.39 | 97.67 |
|   Dry Shrubs  | 53.02 | 53.58 |
|  Green Shrubs |  0.0  |  0.0  |
|     Canopy    | 33.52 | 37.46 |
|  Wood Pieces  | 92.12 | 99.42 |
|   Litterfall  |  nan  |  nan  |
| Timber Litter | 46.91 | 47.06 |
|  Live Trunks  |  0.6  |  0.66 |
|   Bare Earth  | 28.91 | 30.89 |
|     People    | 92.36 | 92.66 |
|      Sky      |  nan  |  nan  |
|     Blurry    |  nan  |  nan  |
|   Obstacles   |  nan  |  nan  |
+---------------+-------+-------+
2023-05-26 22:16:29,727 - mmseg - INFO - Summary:
2023-05-26 22:16:29,728 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 93.33 | 43.48 | 45.94 |
+-------+-------+-------+
2023-05-26 22:16:29,728 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 22:16:29,728 - mmseg - INFO - Iter(val) [11]	aAcc: 0.9333, mIoU: 0.4348, mAcc: 0.4594, IoU.Dry Grass: 0.0000, IoU.Green Grass: 0.8739, IoU.Dry Shrubs: 0.5302, IoU.Green Shrubs: 0.0000, IoU.Canopy: 0.3352, IoU.Wood Pieces: 0.9212, IoU.Litterfall: nan, IoU.Timber Litter: 0.4691, IoU.Live Trunks: 0.0060, IoU.Bare Earth: 0.2891, IoU.People: 0.9236, IoU.Sky: nan, IoU.Blurry: nan, IoU.Obstacles: nan, Acc.Dry Grass: 0.0000, Acc.Green Grass: 0.9767, Acc.Dry Shrubs: 0.5358, Acc.Green Shrubs: 0.0000, Acc.Canopy: 0.3746, Acc.Wood Pieces: 0.9942, Acc.Litterfall: nan, Acc.Timber Litter: 0.4706, Acc.Live Trunks: 0.0066, Acc.Bare Earth: 0.3089, Acc.People: 0.9266, Acc.Sky: nan, Acc.Blurry: nan, Acc.Obstacles: nan
2023-05-26 22:16:51,772 - mmseg - INFO - Iter [8050/20000]	lr: 3.585e-05, eta: 1:21:36, time: 0.497, data_time: 0.258, memory: 3387, decode.loss_ce: 0.1169, decode.acc_seg: 50.0187, loss: 0.1169
2023-05-26 22:17:10,917 - mmseg - INFO - Iter [8100/20000]	lr: 3.570e-05, eta: 1:21:14, time: 0.383, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1141, decode.acc_seg: 49.3048, loss: 0.1141
2023-05-26 22:17:30,441 - mmseg - INFO - Iter [8150/20000]	lr: 3.555e-05, eta: 1:20:52, time: 0.390, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1233, decode.acc_seg: 49.2776, loss: 0.1233
2023-05-26 22:17:50,627 - mmseg - INFO - Iter [8200/20000]	lr: 3.540e-05, eta: 1:20:31, time: 0.404, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1146, decode.acc_seg: 48.4782, loss: 0.1146
2023-05-26 22:18:10,754 - mmseg - INFO - Iter [8250/20000]	lr: 3.525e-05, eta: 1:20:10, time: 0.403, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1237, decode.acc_seg: 49.9086, loss: 0.1237
2023-05-26 22:18:30,874 - mmseg - INFO - Iter [8300/20000]	lr: 3.510e-05, eta: 1:19:49, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1067, decode.acc_seg: 48.7220, loss: 0.1067
2023-05-26 22:18:50,346 - mmseg - INFO - Iter [8350/20000]	lr: 3.495e-05, eta: 1:19:27, time: 0.389, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1118, decode.acc_seg: 51.6307, loss: 0.1118
2023-05-26 22:19:09,622 - mmseg - INFO - Iter [8400/20000]	lr: 3.480e-05, eta: 1:19:05, time: 0.386, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1200, decode.acc_seg: 48.2825, loss: 0.1200
2023-05-26 22:19:32,249 - mmseg - INFO - Iter [8450/20000]	lr: 3.465e-05, eta: 1:18:47, time: 0.452, data_time: 0.202, memory: 3387, decode.loss_ce: 0.1003, decode.acc_seg: 51.6312, loss: 0.1003
2023-05-26 22:19:52,443 - mmseg - INFO - Iter [8500/20000]	lr: 3.450e-05, eta: 1:18:27, time: 0.404, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1067, decode.acc_seg: 50.4044, loss: 0.1067
2023-05-26 22:20:12,581 - mmseg - INFO - Iter [8550/20000]	lr: 3.435e-05, eta: 1:18:06, time: 0.403, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1116, decode.acc_seg: 47.3775, loss: 0.1116
2023-05-26 22:20:32,624 - mmseg - INFO - Iter [8600/20000]	lr: 3.420e-05, eta: 1:17:45, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1022, decode.acc_seg: 47.0743, loss: 0.1022
2023-05-26 22:20:52,663 - mmseg - INFO - Iter [8650/20000]	lr: 3.405e-05, eta: 1:17:24, time: 0.401, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1097, decode.acc_seg: 51.5774, loss: 0.1097
2023-05-26 22:21:12,744 - mmseg - INFO - Iter [8700/20000]	lr: 3.390e-05, eta: 1:17:03, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1212, decode.acc_seg: 48.4298, loss: 0.1212
2023-05-26 22:21:32,803 - mmseg - INFO - Iter [8750/20000]	lr: 3.375e-05, eta: 1:16:42, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1207, decode.acc_seg: 50.5605, loss: 0.1207
2023-05-26 22:21:52,854 - mmseg - INFO - Iter [8800/20000]	lr: 3.360e-05, eta: 1:16:21, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1144, decode.acc_seg: 49.6649, loss: 0.1144
2023-05-26 22:22:15,357 - mmseg - INFO - Iter [8850/20000]	lr: 3.345e-05, eta: 1:16:03, time: 0.450, data_time: 0.201, memory: 3387, decode.loss_ce: 0.1093, decode.acc_seg: 49.9953, loss: 0.1093
2023-05-26 22:22:35,520 - mmseg - INFO - Iter [8900/20000]	lr: 3.330e-05, eta: 1:15:42, time: 0.403, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1450, decode.acc_seg: 47.8468, loss: 0.1450
2023-05-26 22:22:55,727 - mmseg - INFO - Iter [8950/20000]	lr: 3.315e-05, eta: 1:15:21, time: 0.404, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1231, decode.acc_seg: 49.9453, loss: 0.1231
2023-05-26 22:23:15,579 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 22:23:15,580 - mmseg - INFO - Iter [9000/20000]	lr: 3.300e-05, eta: 1:15:00, time: 0.397, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0953, decode.acc_seg: 51.3071, loss: 0.0953
2023-05-26 22:23:34,669 - mmseg - INFO - Iter [9050/20000]	lr: 3.285e-05, eta: 1:14:38, time: 0.382, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1157, decode.acc_seg: 51.0020, loss: 0.1157
2023-05-26 22:23:53,757 - mmseg - INFO - Iter [9100/20000]	lr: 3.270e-05, eta: 1:14:16, time: 0.382, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0927, decode.acc_seg: 49.8814, loss: 0.0927
2023-05-26 22:24:12,881 - mmseg - INFO - Iter [9150/20000]	lr: 3.255e-05, eta: 1:13:54, time: 0.382, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1159, decode.acc_seg: 49.4795, loss: 0.1159
2023-05-26 22:24:32,231 - mmseg - INFO - Iter [9200/20000]	lr: 3.240e-05, eta: 1:13:32, time: 0.387, data_time: 0.154, memory: 3387, decode.loss_ce: 0.1060, decode.acc_seg: 50.6424, loss: 0.1060
2023-05-26 22:24:54,949 - mmseg - INFO - Iter [9250/20000]	lr: 3.225e-05, eta: 1:13:14, time: 0.454, data_time: 0.203, memory: 3387, decode.loss_ce: 0.1012, decode.acc_seg: 50.9062, loss: 0.1012
2023-05-26 22:25:15,072 - mmseg - INFO - Iter [9300/20000]	lr: 3.210e-05, eta: 1:12:54, time: 0.402, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1122, decode.acc_seg: 49.1928, loss: 0.1122
2023-05-26 22:25:35,220 - mmseg - INFO - Iter [9350/20000]	lr: 3.195e-05, eta: 1:12:33, time: 0.403, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1011, decode.acc_seg: 50.4027, loss: 0.1011
2023-05-26 22:25:55,280 - mmseg - INFO - Iter [9400/20000]	lr: 3.180e-05, eta: 1:12:12, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1005, decode.acc_seg: 49.2960, loss: 0.1005
2023-05-26 22:26:15,375 - mmseg - INFO - Iter [9450/20000]	lr: 3.165e-05, eta: 1:11:51, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1075, decode.acc_seg: 51.6620, loss: 0.1075
2023-05-26 22:26:35,496 - mmseg - INFO - Iter [9500/20000]	lr: 3.150e-05, eta: 1:11:30, time: 0.402, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1016, decode.acc_seg: 49.7051, loss: 0.1016
2023-05-26 22:26:55,584 - mmseg - INFO - Iter [9550/20000]	lr: 3.135e-05, eta: 1:11:10, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0928, decode.acc_seg: 49.8355, loss: 0.0928
2023-05-26 22:27:15,788 - mmseg - INFO - Iter [9600/20000]	lr: 3.120e-05, eta: 1:10:49, time: 0.404, data_time: 0.155, memory: 3387, decode.loss_ce: 0.0974, decode.acc_seg: 50.3615, loss: 0.0974
2023-05-26 22:27:38,370 - mmseg - INFO - Iter [9650/20000]	lr: 3.105e-05, eta: 1:10:31, time: 0.452, data_time: 0.201, memory: 3387, decode.loss_ce: 0.1105, decode.acc_seg: 49.0338, loss: 0.1105
2023-05-26 22:27:58,122 - mmseg - INFO - Iter [9700/20000]	lr: 3.090e-05, eta: 1:10:10, time: 0.395, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0985, decode.acc_seg: 49.9086, loss: 0.0985
2023-05-26 22:28:17,185 - mmseg - INFO - Iter [9750/20000]	lr: 3.075e-05, eta: 1:09:48, time: 0.381, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1089, decode.acc_seg: 48.1298, loss: 0.1089
2023-05-26 22:28:36,281 - mmseg - INFO - Iter [9800/20000]	lr: 3.060e-05, eta: 1:09:26, time: 0.382, data_time: 0.154, memory: 3387, decode.loss_ce: 0.1066, decode.acc_seg: 50.1840, loss: 0.1066
2023-05-26 22:28:55,732 - mmseg - INFO - Iter [9850/20000]	lr: 3.045e-05, eta: 1:09:04, time: 0.389, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0957, decode.acc_seg: 49.3938, loss: 0.0957
2023-05-26 22:29:14,959 - mmseg - INFO - Iter [9900/20000]	lr: 3.030e-05, eta: 1:08:43, time: 0.385, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0936, decode.acc_seg: 50.6875, loss: 0.0936
2023-05-26 22:29:35,268 - mmseg - INFO - Iter [9950/20000]	lr: 3.015e-05, eta: 1:08:22, time: 0.406, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0908, decode.acc_seg: 46.7966, loss: 0.0908
2023-05-26 22:29:55,585 - mmseg - INFO - Saving checkpoint at 10000 iterations
2023-05-26 22:29:58,530 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 22:29:58,530 - mmseg - INFO - Iter [10000/20000]	lr: 3.000e-05, eta: 1:08:05, time: 0.465, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0799, decode.acc_seg: 50.8217, loss: 0.0799
2023-05-26 22:30:01,084 - mmseg - INFO - per class results:
2023-05-26 22:30:01,085 - mmseg - INFO - 
+---------------+-------+-------+
|     Class     |  IoU  |  Acc  |
+---------------+-------+-------+
|   Dry Grass   |  0.0  |  0.0  |
|  Green Grass  | 91.46 | 97.55 |
|   Dry Shrubs  | 62.37 | 63.03 |
|  Green Shrubs | 12.98 | 12.98 |
|     Canopy    | 56.34 | 58.07 |
|  Wood Pieces  | 94.89 | 99.58 |
|   Litterfall  |  nan  |  nan  |
| Timber Litter | 66.15 | 72.52 |
|  Live Trunks  |  3.59 |  3.76 |
|   Bare Earth  | 77.44 | 77.46 |
|     People    | 95.82 | 97.21 |
|      Sky      |  nan  |  nan  |
|     Blurry    |  nan  |  nan  |
|   Obstacles   |  nan  |  nan  |
+---------------+-------+-------+
2023-05-26 22:30:01,085 - mmseg - INFO - Summary:
2023-05-26 22:30:01,086 - mmseg - INFO - 
+-------+------+-------+
|  aAcc | mIoU |  mAcc |
+-------+------+-------+
| 95.61 | 56.1 | 58.22 |
+-------+------+-------+
2023-05-26 22:30:01,086 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 22:30:01,086 - mmseg - INFO - Iter(val) [11]	aAcc: 0.9561, mIoU: 0.5610, mAcc: 0.5822, IoU.Dry Grass: 0.0000, IoU.Green Grass: 0.9146, IoU.Dry Shrubs: 0.6237, IoU.Green Shrubs: 0.1298, IoU.Canopy: 0.5634, IoU.Wood Pieces: 0.9489, IoU.Litterfall: nan, IoU.Timber Litter: 0.6615, IoU.Live Trunks: 0.0359, IoU.Bare Earth: 0.7744, IoU.People: 0.9582, IoU.Sky: nan, IoU.Blurry: nan, IoU.Obstacles: nan, Acc.Dry Grass: 0.0000, Acc.Green Grass: 0.9755, Acc.Dry Shrubs: 0.6303, Acc.Green Shrubs: 0.1298, Acc.Canopy: 0.5807, Acc.Wood Pieces: 0.9958, Acc.Litterfall: nan, Acc.Timber Litter: 0.7252, Acc.Live Trunks: 0.0376, Acc.Bare Earth: 0.7746, Acc.People: 0.9721, Acc.Sky: nan, Acc.Blurry: nan, Acc.Obstacles: nan
2023-05-26 22:30:23,145 - mmseg - INFO - Iter [10050/20000]	lr: 2.985e-05, eta: 1:07:48, time: 0.492, data_time: 0.253, memory: 3387, decode.loss_ce: 0.0899, decode.acc_seg: 50.3098, loss: 0.0899
2023-05-26 22:30:43,339 - mmseg - INFO - Iter [10100/20000]	lr: 2.970e-05, eta: 1:07:28, time: 0.404, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1007, decode.acc_seg: 51.7865, loss: 0.1007
2023-05-26 22:31:02,535 - mmseg - INFO - Iter [10150/20000]	lr: 2.955e-05, eta: 1:07:06, time: 0.384, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0855, decode.acc_seg: 48.3590, loss: 0.0855
2023-05-26 22:31:21,765 - mmseg - INFO - Iter [10200/20000]	lr: 2.940e-05, eta: 1:06:44, time: 0.385, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1007, decode.acc_seg: 47.8338, loss: 0.1007
2023-05-26 22:31:40,847 - mmseg - INFO - Iter [10250/20000]	lr: 2.925e-05, eta: 1:06:23, time: 0.382, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0912, decode.acc_seg: 50.5260, loss: 0.0912
2023-05-26 22:32:00,554 - mmseg - INFO - Iter [10300/20000]	lr: 2.910e-05, eta: 1:06:02, time: 0.394, data_time: 0.154, memory: 3387, decode.loss_ce: 0.0856, decode.acc_seg: 50.7864, loss: 0.0856
2023-05-26 22:32:20,448 - mmseg - INFO - Iter [10350/20000]	lr: 2.895e-05, eta: 1:05:41, time: 0.398, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0943, decode.acc_seg: 51.5331, loss: 0.0943
2023-05-26 22:32:39,467 - mmseg - INFO - Iter [10400/20000]	lr: 2.880e-05, eta: 1:05:19, time: 0.380, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0943, decode.acc_seg: 48.7479, loss: 0.0943
2023-05-26 22:33:01,012 - mmseg - INFO - Iter [10450/20000]	lr: 2.865e-05, eta: 1:05:00, time: 0.431, data_time: 0.201, memory: 3387, decode.loss_ce: 0.0908, decode.acc_seg: 49.6614, loss: 0.0908
2023-05-26 22:33:20,038 - mmseg - INFO - Iter [10500/20000]	lr: 2.850e-05, eta: 1:04:38, time: 0.380, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0905, decode.acc_seg: 51.9820, loss: 0.0905
2023-05-26 22:33:39,107 - mmseg - INFO - Iter [10550/20000]	lr: 2.835e-05, eta: 1:04:16, time: 0.381, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0842, decode.acc_seg: 52.9754, loss: 0.0842
2023-05-26 22:33:58,113 - mmseg - INFO - Iter [10600/20000]	lr: 2.820e-05, eta: 1:03:55, time: 0.380, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1133, decode.acc_seg: 48.7284, loss: 0.1133
2023-05-26 22:34:17,166 - mmseg - INFO - Iter [10650/20000]	lr: 2.805e-05, eta: 1:03:33, time: 0.381, data_time: 0.152, memory: 3387, decode.loss_ce: 0.1330, decode.acc_seg: 51.4402, loss: 0.1330
2023-05-26 22:34:36,210 - mmseg - INFO - Iter [10700/20000]	lr: 2.790e-05, eta: 1:03:12, time: 0.381, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1351, decode.acc_seg: 46.3079, loss: 0.1351
2023-05-26 22:34:56,179 - mmseg - INFO - Iter [10750/20000]	lr: 2.775e-05, eta: 1:02:51, time: 0.399, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0933, decode.acc_seg: 52.8043, loss: 0.0933
2023-05-26 22:35:16,314 - mmseg - INFO - Iter [10800/20000]	lr: 2.760e-05, eta: 1:02:30, time: 0.403, data_time: 0.153, memory: 3387, decode.loss_ce: 0.1003, decode.acc_seg: 52.3096, loss: 0.1003
2023-05-26 22:35:38,844 - mmseg - INFO - Iter [10850/20000]	lr: 2.745e-05, eta: 1:02:12, time: 0.451, data_time: 0.201, memory: 3387, decode.loss_ce: 0.0790, decode.acc_seg: 50.1151, loss: 0.0790
2023-05-26 22:35:58,945 - mmseg - INFO - Iter [10900/20000]	lr: 2.730e-05, eta: 1:01:51, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0758, decode.acc_seg: 50.0822, loss: 0.0758
2023-05-26 22:36:18,906 - mmseg - INFO - Iter [10950/20000]	lr: 2.715e-05, eta: 1:01:30, time: 0.399, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0828, decode.acc_seg: 50.6137, loss: 0.0828
2023-05-26 22:36:38,909 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 22:36:38,909 - mmseg - INFO - Iter [11000/20000]	lr: 2.700e-05, eta: 1:01:10, time: 0.400, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0939, decode.acc_seg: 51.7998, loss: 0.0939
2023-05-26 22:36:58,916 - mmseg - INFO - Iter [11050/20000]	lr: 2.685e-05, eta: 1:00:49, time: 0.400, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0853, decode.acc_seg: 51.4230, loss: 0.0853
2023-05-26 22:37:18,902 - mmseg - INFO - Iter [11100/20000]	lr: 2.670e-05, eta: 1:00:28, time: 0.400, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0805, decode.acc_seg: 50.3123, loss: 0.0805
2023-05-26 22:37:38,930 - mmseg - INFO - Iter [11150/20000]	lr: 2.655e-05, eta: 1:00:07, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0830, decode.acc_seg: 50.0769, loss: 0.0830
2023-05-26 22:37:59,167 - mmseg - INFO - Iter [11200/20000]	lr: 2.640e-05, eta: 0:59:47, time: 0.405, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0947, decode.acc_seg: 52.8465, loss: 0.0947
2023-05-26 22:38:21,813 - mmseg - INFO - Iter [11250/20000]	lr: 2.625e-05, eta: 0:59:28, time: 0.453, data_time: 0.202, memory: 3387, decode.loss_ce: 0.0854, decode.acc_seg: 50.2354, loss: 0.0854
2023-05-26 22:38:41,964 - mmseg - INFO - Iter [11300/20000]	lr: 2.610e-05, eta: 0:59:08, time: 0.403, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0797, decode.acc_seg: 52.2725, loss: 0.0797
2023-05-26 22:39:02,149 - mmseg - INFO - Iter [11350/20000]	lr: 2.595e-05, eta: 0:58:47, time: 0.404, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0714, decode.acc_seg: 50.7258, loss: 0.0714
2023-05-26 22:39:21,913 - mmseg - INFO - Iter [11400/20000]	lr: 2.580e-05, eta: 0:58:26, time: 0.395, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0732, decode.acc_seg: 51.0582, loss: 0.0732
2023-05-26 22:39:41,029 - mmseg - INFO - Iter [11450/20000]	lr: 2.565e-05, eta: 0:58:05, time: 0.382, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0794, decode.acc_seg: 53.4518, loss: 0.0794
2023-05-26 22:40:00,301 - mmseg - INFO - Iter [11500/20000]	lr: 2.550e-05, eta: 0:57:44, time: 0.385, data_time: 0.154, memory: 3387, decode.loss_ce: 0.0820, decode.acc_seg: 52.9330, loss: 0.0820
2023-05-26 22:40:19,939 - mmseg - INFO - Iter [11550/20000]	lr: 2.535e-05, eta: 0:57:23, time: 0.393, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0914, decode.acc_seg: 48.8602, loss: 0.0914
2023-05-26 22:40:40,222 - mmseg - INFO - Iter [11600/20000]	lr: 2.520e-05, eta: 0:57:02, time: 0.406, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0939, decode.acc_seg: 52.8540, loss: 0.0939
2023-05-26 22:41:02,841 - mmseg - INFO - Iter [11650/20000]	lr: 2.505e-05, eta: 0:56:44, time: 0.452, data_time: 0.201, memory: 3387, decode.loss_ce: 0.0721, decode.acc_seg: 51.5836, loss: 0.0721
2023-05-26 22:41:22,988 - mmseg - INFO - Iter [11700/20000]	lr: 2.490e-05, eta: 0:56:23, time: 0.403, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0779, decode.acc_seg: 51.5641, loss: 0.0779
2023-05-26 22:41:43,169 - mmseg - INFO - Iter [11750/20000]	lr: 2.475e-05, eta: 0:56:03, time: 0.404, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0844, decode.acc_seg: 49.4290, loss: 0.0844
2023-05-26 22:42:03,362 - mmseg - INFO - Iter [11800/20000]	lr: 2.460e-05, eta: 0:55:42, time: 0.404, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0696, decode.acc_seg: 52.3350, loss: 0.0696
2023-05-26 22:42:23,525 - mmseg - INFO - Iter [11850/20000]	lr: 2.445e-05, eta: 0:55:22, time: 0.403, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0656, decode.acc_seg: 49.6098, loss: 0.0656
2023-05-26 22:42:43,774 - mmseg - INFO - Iter [11900/20000]	lr: 2.430e-05, eta: 0:55:01, time: 0.405, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0685, decode.acc_seg: 53.3788, loss: 0.0685
2023-05-26 22:43:03,983 - mmseg - INFO - Iter [11950/20000]	lr: 2.415e-05, eta: 0:54:41, time: 0.404, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0857, decode.acc_seg: 48.6750, loss: 0.0857
2023-05-26 22:43:24,075 - mmseg - INFO - Saving checkpoint at 12000 iterations
2023-05-26 22:43:26,760 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 22:43:26,761 - mmseg - INFO - Iter [12000/20000]	lr: 2.400e-05, eta: 0:54:22, time: 0.456, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0808, decode.acc_seg: 50.8591, loss: 0.0808
2023-05-26 22:43:29,355 - mmseg - INFO - per class results:
2023-05-26 22:43:29,357 - mmseg - INFO - 
+---------------+-------+-------+
|     Class     |  IoU  |  Acc  |
+---------------+-------+-------+
|   Dry Grass   |  0.0  |  0.0  |
|  Green Grass  | 92.41 |  98.2 |
|   Dry Shrubs  | 69.91 | 73.48 |
|  Green Shrubs | 23.35 | 26.57 |
|     Canopy    | 67.21 | 75.72 |
|  Wood Pieces  | 95.85 | 98.55 |
|   Litterfall  |  nan  |  nan  |
| Timber Litter | 76.56 | 79.59 |
|  Live Trunks  | 38.38 | 38.38 |
|   Bare Earth  |  77.2 |  77.2 |
|     People    | 95.86 | 96.67 |
|      Sky      |  nan  |  nan  |
|     Blurry    |  nan  |  nan  |
|   Obstacles   |  nan  |  nan  |
+---------------+-------+-------+
2023-05-26 22:43:29,357 - mmseg - INFO - Summary:
2023-05-26 22:43:29,357 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 96.14 | 63.67 | 66.44 |
+-------+-------+-------+
2023-05-26 22:43:29,358 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 22:43:29,358 - mmseg - INFO - Iter(val) [11]	aAcc: 0.9614, mIoU: 0.6367, mAcc: 0.6644, IoU.Dry Grass: 0.0000, IoU.Green Grass: 0.9241, IoU.Dry Shrubs: 0.6991, IoU.Green Shrubs: 0.2335, IoU.Canopy: 0.6721, IoU.Wood Pieces: 0.9585, IoU.Litterfall: nan, IoU.Timber Litter: 0.7656, IoU.Live Trunks: 0.3838, IoU.Bare Earth: 0.7720, IoU.People: 0.9586, IoU.Sky: nan, IoU.Blurry: nan, IoU.Obstacles: nan, Acc.Dry Grass: 0.0000, Acc.Green Grass: 0.9820, Acc.Dry Shrubs: 0.7348, Acc.Green Shrubs: 0.2657, Acc.Canopy: 0.7572, Acc.Wood Pieces: 0.9855, Acc.Litterfall: nan, Acc.Timber Litter: 0.7959, Acc.Live Trunks: 0.3838, Acc.Bare Earth: 0.7720, Acc.People: 0.9667, Acc.Sky: nan, Acc.Blurry: nan, Acc.Obstacles: nan
2023-05-26 22:43:52,154 - mmseg - INFO - Iter [12050/20000]	lr: 2.385e-05, eta: 0:54:05, time: 0.508, data_time: 0.254, memory: 3387, decode.loss_ce: 0.0700, decode.acc_seg: 52.6713, loss: 0.0700
2023-05-26 22:44:12,552 - mmseg - INFO - Iter [12100/20000]	lr: 2.370e-05, eta: 0:53:44, time: 0.408, data_time: 0.154, memory: 3387, decode.loss_ce: 0.0763, decode.acc_seg: 48.6244, loss: 0.0763
2023-05-26 22:44:32,811 - mmseg - INFO - Iter [12150/20000]	lr: 2.355e-05, eta: 0:53:24, time: 0.405, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0785, decode.acc_seg: 53.4523, loss: 0.0785
2023-05-26 22:44:52,997 - mmseg - INFO - Iter [12200/20000]	lr: 2.340e-05, eta: 0:53:03, time: 0.404, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0722, decode.acc_seg: 51.6806, loss: 0.0722
2023-05-26 22:45:12,996 - mmseg - INFO - Iter [12250/20000]	lr: 2.325e-05, eta: 0:52:43, time: 0.400, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0657, decode.acc_seg: 50.2969, loss: 0.0657
2023-05-26 22:45:32,840 - mmseg - INFO - Iter [12300/20000]	lr: 2.310e-05, eta: 0:52:22, time: 0.397, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0803, decode.acc_seg: 54.3283, loss: 0.0803
2023-05-26 22:45:53,064 - mmseg - INFO - Iter [12350/20000]	lr: 2.295e-05, eta: 0:52:01, time: 0.404, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0645, decode.acc_seg: 50.0468, loss: 0.0645
2023-05-26 22:46:13,360 - mmseg - INFO - Iter [12400/20000]	lr: 2.280e-05, eta: 0:51:41, time: 0.406, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0709, decode.acc_seg: 51.0537, loss: 0.0709
2023-05-26 22:46:35,027 - mmseg - INFO - Iter [12450/20000]	lr: 2.265e-05, eta: 0:51:21, time: 0.433, data_time: 0.202, memory: 3387, decode.loss_ce: 0.0723, decode.acc_seg: 50.7174, loss: 0.0723
2023-05-26 22:46:54,711 - mmseg - INFO - Iter [12500/20000]	lr: 2.250e-05, eta: 0:51:00, time: 0.394, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0641, decode.acc_seg: 49.3458, loss: 0.0641
2023-05-26 22:47:14,829 - mmseg - INFO - Iter [12550/20000]	lr: 2.235e-05, eta: 0:50:40, time: 0.402, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0574, decode.acc_seg: 53.1485, loss: 0.0574
2023-05-26 22:47:34,628 - mmseg - INFO - Iter [12600/20000]	lr: 2.220e-05, eta: 0:50:19, time: 0.396, data_time: 0.154, memory: 3387, decode.loss_ce: 0.1291, decode.acc_seg: 50.9684, loss: 0.1291
2023-05-26 22:47:53,759 - mmseg - INFO - Iter [12650/20000]	lr: 2.205e-05, eta: 0:49:58, time: 0.383, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0991, decode.acc_seg: 51.3898, loss: 0.0991
2023-05-26 22:48:12,811 - mmseg - INFO - Iter [12700/20000]	lr: 2.190e-05, eta: 0:49:37, time: 0.381, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0746, decode.acc_seg: 51.6317, loss: 0.0746
2023-05-26 22:48:31,877 - mmseg - INFO - Iter [12750/20000]	lr: 2.175e-05, eta: 0:49:16, time: 0.381, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0603, decode.acc_seg: 49.2693, loss: 0.0603
2023-05-26 22:48:50,901 - mmseg - INFO - Iter [12800/20000]	lr: 2.160e-05, eta: 0:48:54, time: 0.380, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0726, decode.acc_seg: 50.0822, loss: 0.0726
2023-05-26 22:49:12,417 - mmseg - INFO - Iter [12850/20000]	lr: 2.145e-05, eta: 0:48:35, time: 0.430, data_time: 0.201, memory: 3387, decode.loss_ce: 0.0834, decode.acc_seg: 49.2347, loss: 0.0834
2023-05-26 22:49:32,075 - mmseg - INFO - Iter [12900/20000]	lr: 2.130e-05, eta: 0:48:14, time: 0.393, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0702, decode.acc_seg: 52.2965, loss: 0.0702
2023-05-26 22:49:52,211 - mmseg - INFO - Iter [12950/20000]	lr: 2.115e-05, eta: 0:47:53, time: 0.403, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0642, decode.acc_seg: 53.6653, loss: 0.0642
2023-05-26 22:50:12,370 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 22:50:12,370 - mmseg - INFO - Iter [13000/20000]	lr: 2.100e-05, eta: 0:47:33, time: 0.403, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0732, decode.acc_seg: 52.2023, loss: 0.0732
2023-05-26 22:50:32,526 - mmseg - INFO - Iter [13050/20000]	lr: 2.085e-05, eta: 0:47:12, time: 0.403, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0607, decode.acc_seg: 49.1709, loss: 0.0607
2023-05-26 22:50:52,737 - mmseg - INFO - Iter [13100/20000]	lr: 2.070e-05, eta: 0:46:52, time: 0.404, data_time: 0.155, memory: 3387, decode.loss_ce: 0.0933, decode.acc_seg: 51.1339, loss: 0.0933
2023-05-26 22:51:12,837 - mmseg - INFO - Iter [13150/20000]	lr: 2.055e-05, eta: 0:46:31, time: 0.402, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0905, decode.acc_seg: 49.6135, loss: 0.0905
2023-05-26 22:51:32,873 - mmseg - INFO - Iter [13200/20000]	lr: 2.040e-05, eta: 0:46:11, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0735, decode.acc_seg: 50.9003, loss: 0.0735
2023-05-26 22:51:55,479 - mmseg - INFO - Iter [13250/20000]	lr: 2.025e-05, eta: 0:45:52, time: 0.452, data_time: 0.202, memory: 3387, decode.loss_ce: 0.0670, decode.acc_seg: 53.5769, loss: 0.0670
2023-05-26 22:52:15,549 - mmseg - INFO - Iter [13300/20000]	lr: 2.010e-05, eta: 0:45:31, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0738, decode.acc_seg: 48.1552, loss: 0.0738
2023-05-26 22:52:35,599 - mmseg - INFO - Iter [13350/20000]	lr: 1.995e-05, eta: 0:45:10, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0606, decode.acc_seg: 49.7940, loss: 0.0606
2023-05-26 22:52:55,637 - mmseg - INFO - Iter [13400/20000]	lr: 1.980e-05, eta: 0:44:50, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0677, decode.acc_seg: 52.0984, loss: 0.0677
2023-05-26 22:53:15,697 - mmseg - INFO - Iter [13450/20000]	lr: 1.965e-05, eta: 0:44:29, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0599, decode.acc_seg: 53.0814, loss: 0.0599
2023-05-26 22:53:35,734 - mmseg - INFO - Iter [13500/20000]	lr: 1.950e-05, eta: 0:44:09, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0511, decode.acc_seg: 53.9867, loss: 0.0511
2023-05-26 22:53:55,752 - mmseg - INFO - Iter [13550/20000]	lr: 1.935e-05, eta: 0:43:48, time: 0.400, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0561, decode.acc_seg: 51.1003, loss: 0.0561
2023-05-26 22:54:15,831 - mmseg - INFO - Iter [13600/20000]	lr: 1.920e-05, eta: 0:43:28, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0585, decode.acc_seg: 51.0570, loss: 0.0585
2023-05-26 22:54:38,335 - mmseg - INFO - Iter [13650/20000]	lr: 1.905e-05, eta: 0:43:08, time: 0.450, data_time: 0.201, memory: 3387, decode.loss_ce: 0.0592, decode.acc_seg: 52.3088, loss: 0.0592
2023-05-26 22:54:58,519 - mmseg - INFO - Iter [13700/20000]	lr: 1.890e-05, eta: 0:42:48, time: 0.404, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0519, decode.acc_seg: 54.3024, loss: 0.0519
2023-05-26 22:55:18,608 - mmseg - INFO - Iter [13750/20000]	lr: 1.875e-05, eta: 0:42:27, time: 0.402, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0664, decode.acc_seg: 48.9145, loss: 0.0664
2023-05-26 22:55:38,701 - mmseg - INFO - Iter [13800/20000]	lr: 1.860e-05, eta: 0:42:07, time: 0.402, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0569, decode.acc_seg: 50.2036, loss: 0.0569
2023-05-26 22:55:58,749 - mmseg - INFO - Iter [13850/20000]	lr: 1.845e-05, eta: 0:41:46, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0659, decode.acc_seg: 52.5350, loss: 0.0659
2023-05-26 22:56:18,865 - mmseg - INFO - Iter [13900/20000]	lr: 1.830e-05, eta: 0:41:26, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0647, decode.acc_seg: 52.7282, loss: 0.0647
2023-05-26 22:56:39,084 - mmseg - INFO - Iter [13950/20000]	lr: 1.815e-05, eta: 0:41:05, time: 0.404, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0607, decode.acc_seg: 50.9875, loss: 0.0607
2023-05-26 22:56:59,351 - mmseg - INFO - Saving checkpoint at 14000 iterations
2023-05-26 22:57:01,296 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 22:57:01,296 - mmseg - INFO - Iter [14000/20000]	lr: 1.800e-05, eta: 0:40:46, time: 0.444, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0796, decode.acc_seg: 53.0789, loss: 0.0796
2023-05-26 22:57:03,995 - mmseg - INFO - per class results:
2023-05-26 22:57:03,997 - mmseg - INFO - 
+---------------+-------+-------+
|     Class     |  IoU  |  Acc  |
+---------------+-------+-------+
|   Dry Grass   |  0.0  |  0.0  |
|  Green Grass  | 93.96 | 97.46 |
|   Dry Shrubs  | 77.25 | 84.83 |
|  Green Shrubs | 37.88 | 45.92 |
|     Canopy    | 60.93 | 64.14 |
|  Wood Pieces  | 94.04 | 98.98 |
|   Litterfall  |  nan  |  nan  |
| Timber Litter | 87.24 |  91.5 |
|  Live Trunks  |  0.18 |  0.18 |
|   Bare Earth  | 78.29 | 78.29 |
|     People    | 96.82 | 97.27 |
|      Sky      |  nan  |  nan  |
|     Blurry    |  nan  |  nan  |
|   Obstacles   |  nan  |  nan  |
+---------------+-------+-------+
2023-05-26 22:57:03,997 - mmseg - INFO - Summary:
2023-05-26 22:57:03,997 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 96.48 | 62.66 | 65.86 |
+-------+-------+-------+
2023-05-26 22:57:03,998 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 22:57:03,998 - mmseg - INFO - Iter(val) [11]	aAcc: 0.9648, mIoU: 0.6266, mAcc: 0.6586, IoU.Dry Grass: 0.0000, IoU.Green Grass: 0.9396, IoU.Dry Shrubs: 0.7725, IoU.Green Shrubs: 0.3788, IoU.Canopy: 0.6093, IoU.Wood Pieces: 0.9404, IoU.Litterfall: nan, IoU.Timber Litter: 0.8724, IoU.Live Trunks: 0.0018, IoU.Bare Earth: 0.7829, IoU.People: 0.9682, IoU.Sky: nan, IoU.Blurry: nan, IoU.Obstacles: nan, Acc.Dry Grass: 0.0000, Acc.Green Grass: 0.9746, Acc.Dry Shrubs: 0.8483, Acc.Green Shrubs: 0.4592, Acc.Canopy: 0.6414, Acc.Wood Pieces: 0.9898, Acc.Litterfall: nan, Acc.Timber Litter: 0.9150, Acc.Live Trunks: 0.0018, Acc.Bare Earth: 0.7829, Acc.People: 0.9727, Acc.Sky: nan, Acc.Blurry: nan, Acc.Obstacles: nan
2023-05-26 22:57:26,902 - mmseg - INFO - Iter [14050/20000]	lr: 1.785e-05, eta: 0:40:28, time: 0.512, data_time: 0.258, memory: 3387, decode.loss_ce: 0.0623, decode.acc_seg: 51.7013, loss: 0.0623
2023-05-26 22:57:47,077 - mmseg - INFO - Iter [14100/20000]	lr: 1.770e-05, eta: 0:40:07, time: 0.404, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0690, decode.acc_seg: 50.6432, loss: 0.0690
2023-05-26 22:58:07,179 - mmseg - INFO - Iter [14150/20000]	lr: 1.755e-05, eta: 0:39:47, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0601, decode.acc_seg: 52.9467, loss: 0.0601
2023-05-26 22:58:27,400 - mmseg - INFO - Iter [14200/20000]	lr: 1.740e-05, eta: 0:39:26, time: 0.404, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0582, decode.acc_seg: 51.6300, loss: 0.0582
2023-05-26 22:58:47,621 - mmseg - INFO - Iter [14250/20000]	lr: 1.725e-05, eta: 0:39:06, time: 0.404, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0566, decode.acc_seg: 50.3846, loss: 0.0566
2023-05-26 22:59:08,019 - mmseg - INFO - Iter [14300/20000]	lr: 1.710e-05, eta: 0:38:45, time: 0.408, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0506, decode.acc_seg: 51.1650, loss: 0.0506
2023-05-26 22:59:28,252 - mmseg - INFO - Iter [14350/20000]	lr: 1.695e-05, eta: 0:38:25, time: 0.405, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0540, decode.acc_seg: 50.4417, loss: 0.0540
2023-05-26 22:59:48,328 - mmseg - INFO - Iter [14400/20000]	lr: 1.680e-05, eta: 0:38:04, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0700, decode.acc_seg: 52.6731, loss: 0.0700
2023-05-26 23:00:09,899 - mmseg - INFO - Iter [14450/20000]	lr: 1.665e-05, eta: 0:37:44, time: 0.431, data_time: 0.202, memory: 3387, decode.loss_ce: 0.0665, decode.acc_seg: 52.6425, loss: 0.0665
2023-05-26 23:00:28,940 - mmseg - INFO - Iter [14500/20000]	lr: 1.650e-05, eta: 0:37:23, time: 0.381, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0559, decode.acc_seg: 50.1793, loss: 0.0559
2023-05-26 23:00:48,035 - mmseg - INFO - Iter [14550/20000]	lr: 1.635e-05, eta: 0:37:02, time: 0.382, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0538, decode.acc_seg: 51.5156, loss: 0.0538
2023-05-26 23:01:07,911 - mmseg - INFO - Iter [14600/20000]	lr: 1.620e-05, eta: 0:36:42, time: 0.397, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0522, decode.acc_seg: 51.5759, loss: 0.0522
2023-05-26 23:01:28,133 - mmseg - INFO - Iter [14650/20000]	lr: 1.605e-05, eta: 0:36:21, time: 0.404, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0561, decode.acc_seg: 50.3700, loss: 0.0561
2023-05-26 23:01:48,199 - mmseg - INFO - Iter [14700/20000]	lr: 1.590e-05, eta: 0:36:01, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0712, decode.acc_seg: 50.4699, loss: 0.0712
2023-05-26 23:02:08,272 - mmseg - INFO - Iter [14750/20000]	lr: 1.575e-05, eta: 0:35:40, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0615, decode.acc_seg: 52.2861, loss: 0.0615
2023-05-26 23:02:28,328 - mmseg - INFO - Iter [14800/20000]	lr: 1.560e-05, eta: 0:35:20, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0470, decode.acc_seg: 52.9710, loss: 0.0470
2023-05-26 23:02:50,970 - mmseg - INFO - Iter [14850/20000]	lr: 1.545e-05, eta: 0:35:00, time: 0.453, data_time: 0.202, memory: 3387, decode.loss_ce: 0.0501, decode.acc_seg: 51.7383, loss: 0.0501
2023-05-26 23:03:11,146 - mmseg - INFO - Iter [14900/20000]	lr: 1.530e-05, eta: 0:34:40, time: 0.403, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0516, decode.acc_seg: 51.1486, loss: 0.0516
2023-05-26 23:03:30,457 - mmseg - INFO - Iter [14950/20000]	lr: 1.515e-05, eta: 0:34:19, time: 0.386, data_time: 0.154, memory: 3387, decode.loss_ce: 0.0595, decode.acc_seg: 51.3367, loss: 0.0595
2023-05-26 23:03:49,699 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 23:03:49,699 - mmseg - INFO - Iter [15000/20000]	lr: 1.500e-05, eta: 0:33:58, time: 0.385, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0596, decode.acc_seg: 53.2694, loss: 0.0596
2023-05-26 23:04:09,920 - mmseg - INFO - Iter [15050/20000]	lr: 1.485e-05, eta: 0:33:38, time: 0.404, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0553, decode.acc_seg: 51.3401, loss: 0.0553
2023-05-26 23:04:30,086 - mmseg - INFO - Iter [15100/20000]	lr: 1.470e-05, eta: 0:33:17, time: 0.403, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0499, decode.acc_seg: 52.0568, loss: 0.0499
2023-05-26 23:04:50,212 - mmseg - INFO - Iter [15150/20000]	lr: 1.455e-05, eta: 0:32:57, time: 0.403, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0546, decode.acc_seg: 50.6883, loss: 0.0546
2023-05-26 23:05:10,489 - mmseg - INFO - Iter [15200/20000]	lr: 1.440e-05, eta: 0:32:36, time: 0.406, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0432, decode.acc_seg: 50.7692, loss: 0.0432
2023-05-26 23:05:33,097 - mmseg - INFO - Iter [15250/20000]	lr: 1.425e-05, eta: 0:32:17, time: 0.452, data_time: 0.201, memory: 3387, decode.loss_ce: 0.0530, decode.acc_seg: 50.0297, loss: 0.0530
2023-05-26 23:05:53,271 - mmseg - INFO - Iter [15300/20000]	lr: 1.410e-05, eta: 0:31:56, time: 0.403, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0715, decode.acc_seg: 52.0205, loss: 0.0715
2023-05-26 23:06:13,411 - mmseg - INFO - Iter [15350/20000]	lr: 1.395e-05, eta: 0:31:36, time: 0.403, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0489, decode.acc_seg: 53.5343, loss: 0.0489
2023-05-26 23:06:33,498 - mmseg - INFO - Iter [15400/20000]	lr: 1.380e-05, eta: 0:31:15, time: 0.402, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0496, decode.acc_seg: 48.3732, loss: 0.0496
2023-05-26 23:06:53,551 - mmseg - INFO - Iter [15450/20000]	lr: 1.365e-05, eta: 0:30:55, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0519, decode.acc_seg: 52.0386, loss: 0.0519
2023-05-26 23:07:13,708 - mmseg - INFO - Iter [15500/20000]	lr: 1.350e-05, eta: 0:30:34, time: 0.403, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0550, decode.acc_seg: 50.4158, loss: 0.0550
2023-05-26 23:07:33,793 - mmseg - INFO - Iter [15550/20000]	lr: 1.335e-05, eta: 0:30:14, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0551, decode.acc_seg: 54.1862, loss: 0.0551
2023-05-26 23:07:54,021 - mmseg - INFO - Iter [15600/20000]	lr: 1.320e-05, eta: 0:29:54, time: 0.405, data_time: 0.155, memory: 3387, decode.loss_ce: 0.0522, decode.acc_seg: 50.6999, loss: 0.0522
2023-05-26 23:08:16,574 - mmseg - INFO - Iter [15650/20000]	lr: 1.305e-05, eta: 0:29:34, time: 0.451, data_time: 0.202, memory: 3387, decode.loss_ce: 0.0565, decode.acc_seg: 52.6620, loss: 0.0565
2023-05-26 23:08:36,631 - mmseg - INFO - Iter [15700/20000]	lr: 1.290e-05, eta: 0:29:13, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0564, decode.acc_seg: 51.7410, loss: 0.0564
2023-05-26 23:08:56,665 - mmseg - INFO - Iter [15750/20000]	lr: 1.275e-05, eta: 0:28:53, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0476, decode.acc_seg: 51.4359, loss: 0.0476
2023-05-26 23:09:16,696 - mmseg - INFO - Iter [15800/20000]	lr: 1.260e-05, eta: 0:28:32, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0517, decode.acc_seg: 52.0193, loss: 0.0517
2023-05-26 23:09:36,801 - mmseg - INFO - Iter [15850/20000]	lr: 1.245e-05, eta: 0:28:12, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0463, decode.acc_seg: 52.8914, loss: 0.0463
2023-05-26 23:09:56,836 - mmseg - INFO - Iter [15900/20000]	lr: 1.230e-05, eta: 0:27:51, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0503, decode.acc_seg: 49.8214, loss: 0.0503
2023-05-26 23:10:16,943 - mmseg - INFO - Iter [15950/20000]	lr: 1.215e-05, eta: 0:27:31, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0394, decode.acc_seg: 51.6151, loss: 0.0394
2023-05-26 23:10:36,837 - mmseg - INFO - Saving checkpoint at 16000 iterations
2023-05-26 23:10:38,923 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 23:10:38,924 - mmseg - INFO - Iter [16000/20000]	lr: 1.200e-05, eta: 0:27:11, time: 0.440, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0483, decode.acc_seg: 52.5174, loss: 0.0483
2023-05-26 23:10:41,226 - mmseg - INFO - per class results:
2023-05-26 23:10:41,228 - mmseg - INFO - 
+---------------+-------+-------+
|     Class     |  IoU  |  Acc  |
+---------------+-------+-------+
|   Dry Grass   |  0.0  |  0.0  |
|  Green Grass  | 95.72 | 98.66 |
|   Dry Shrubs  | 79.91 |  87.9 |
|  Green Shrubs | 43.72 |  44.6 |
|     Canopy    | 75.32 | 84.43 |
|  Wood Pieces  | 96.57 | 98.01 |
|   Litterfall  |  nan  |  nan  |
| Timber Litter | 97.11 | 97.49 |
|  Live Trunks  | 68.82 | 69.04 |
|   Bare Earth  | 92.36 | 92.36 |
|     People    | 97.49 | 98.04 |
|      Sky      |  nan  |  nan  |
|     Blurry    |  nan  |  nan  |
|   Obstacles   |  nan  |  nan  |
+---------------+-------+-------+
2023-05-26 23:10:41,228 - mmseg - INFO - Summary:
2023-05-26 23:10:41,228 - mmseg - INFO - 
+-------+------+-------+
|  aAcc | mIoU |  mAcc |
+-------+------+-------+
| 97.42 | 74.7 | 77.05 |
+-------+------+-------+
2023-05-26 23:10:41,229 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 23:10:41,229 - mmseg - INFO - Iter(val) [11]	aAcc: 0.9742, mIoU: 0.7470, mAcc: 0.7705, IoU.Dry Grass: 0.0000, IoU.Green Grass: 0.9572, IoU.Dry Shrubs: 0.7991, IoU.Green Shrubs: 0.4372, IoU.Canopy: 0.7532, IoU.Wood Pieces: 0.9657, IoU.Litterfall: nan, IoU.Timber Litter: 0.9711, IoU.Live Trunks: 0.6882, IoU.Bare Earth: 0.9236, IoU.People: 0.9749, IoU.Sky: nan, IoU.Blurry: nan, IoU.Obstacles: nan, Acc.Dry Grass: 0.0000, Acc.Green Grass: 0.9866, Acc.Dry Shrubs: 0.8790, Acc.Green Shrubs: 0.4460, Acc.Canopy: 0.8443, Acc.Wood Pieces: 0.9801, Acc.Litterfall: nan, Acc.Timber Litter: 0.9749, Acc.Live Trunks: 0.6904, Acc.Bare Earth: 0.9236, Acc.People: 0.9804, Acc.Sky: nan, Acc.Blurry: nan, Acc.Obstacles: nan
2023-05-26 23:11:03,859 - mmseg - INFO - Iter [16050/20000]	lr: 1.185e-05, eta: 0:26:52, time: 0.499, data_time: 0.248, memory: 3387, decode.loss_ce: 0.0492, decode.acc_seg: 51.6605, loss: 0.0492
2023-05-26 23:11:24,001 - mmseg - INFO - Iter [16100/20000]	lr: 1.170e-05, eta: 0:26:31, time: 0.403, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0579, decode.acc_seg: 54.7183, loss: 0.0579
2023-05-26 23:11:44,129 - mmseg - INFO - Iter [16150/20000]	lr: 1.155e-05, eta: 0:26:11, time: 0.403, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0395, decode.acc_seg: 50.2489, loss: 0.0395
2023-05-26 23:12:04,371 - mmseg - INFO - Iter [16200/20000]	lr: 1.140e-05, eta: 0:25:50, time: 0.405, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0394, decode.acc_seg: 52.1292, loss: 0.0394
2023-05-26 23:12:24,486 - mmseg - INFO - Iter [16250/20000]	lr: 1.125e-05, eta: 0:25:30, time: 0.402, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0460, decode.acc_seg: 50.9448, loss: 0.0460
2023-05-26 23:12:44,618 - mmseg - INFO - Iter [16300/20000]	lr: 1.110e-05, eta: 0:25:09, time: 0.403, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0450, decode.acc_seg: 53.0046, loss: 0.0450
2023-05-26 23:13:04,758 - mmseg - INFO - Iter [16350/20000]	lr: 1.095e-05, eta: 0:24:49, time: 0.403, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0539, decode.acc_seg: 53.7598, loss: 0.0539
2023-05-26 23:13:24,870 - mmseg - INFO - Iter [16400/20000]	lr: 1.080e-05, eta: 0:24:28, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0431, decode.acc_seg: 50.4709, loss: 0.0431
2023-05-26 23:13:47,437 - mmseg - INFO - Iter [16450/20000]	lr: 1.065e-05, eta: 0:24:08, time: 0.451, data_time: 0.200, memory: 3387, decode.loss_ce: 0.0465, decode.acc_seg: 51.8615, loss: 0.0465
2023-05-26 23:14:07,489 - mmseg - INFO - Iter [16500/20000]	lr: 1.050e-05, eta: 0:23:48, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0776, decode.acc_seg: 51.5096, loss: 0.0776
2023-05-26 23:14:27,512 - mmseg - INFO - Iter [16550/20000]	lr: 1.035e-05, eta: 0:23:27, time: 0.400, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0468, decode.acc_seg: 52.1977, loss: 0.0468
2023-05-26 23:14:47,579 - mmseg - INFO - Iter [16600/20000]	lr: 1.020e-05, eta: 0:23:07, time: 0.401, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0505, decode.acc_seg: 50.3737, loss: 0.0505
2023-05-26 23:15:07,638 - mmseg - INFO - Iter [16650/20000]	lr: 1.005e-05, eta: 0:22:47, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0433, decode.acc_seg: 49.7989, loss: 0.0433
2023-05-26 23:15:27,751 - mmseg - INFO - Iter [16700/20000]	lr: 9.903e-06, eta: 0:22:26, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0462, decode.acc_seg: 56.3919, loss: 0.0462
2023-05-26 23:15:47,875 - mmseg - INFO - Iter [16750/20000]	lr: 9.753e-06, eta: 0:22:06, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0518, decode.acc_seg: 47.4968, loss: 0.0518
2023-05-26 23:16:07,944 - mmseg - INFO - Iter [16800/20000]	lr: 9.603e-06, eta: 0:21:45, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0687, decode.acc_seg: 52.7641, loss: 0.0687
2023-05-26 23:16:30,399 - mmseg - INFO - Iter [16850/20000]	lr: 9.453e-06, eta: 0:21:25, time: 0.449, data_time: 0.200, memory: 3387, decode.loss_ce: 0.0495, decode.acc_seg: 52.1465, loss: 0.0495
2023-05-26 23:16:50,537 - mmseg - INFO - Iter [16900/20000]	lr: 9.303e-06, eta: 0:21:05, time: 0.403, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0461, decode.acc_seg: 52.4895, loss: 0.0461
2023-05-26 23:17:10,674 - mmseg - INFO - Iter [16950/20000]	lr: 9.153e-06, eta: 0:20:44, time: 0.403, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0558, decode.acc_seg: 54.4075, loss: 0.0558
2023-05-26 23:17:30,889 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 23:17:30,889 - mmseg - INFO - Iter [17000/20000]	lr: 9.003e-06, eta: 0:20:24, time: 0.404, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0503, decode.acc_seg: 51.9695, loss: 0.0503
2023-05-26 23:17:50,967 - mmseg - INFO - Iter [17050/20000]	lr: 8.853e-06, eta: 0:20:03, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0407, decode.acc_seg: 53.7371, loss: 0.0407
2023-05-26 23:18:11,114 - mmseg - INFO - Iter [17100/20000]	lr: 8.703e-06, eta: 0:19:43, time: 0.403, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0348, decode.acc_seg: 50.3267, loss: 0.0348
2023-05-26 23:18:31,124 - mmseg - INFO - Iter [17150/20000]	lr: 8.553e-06, eta: 0:19:22, time: 0.400, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0386, decode.acc_seg: 51.6566, loss: 0.0386
2023-05-26 23:18:51,373 - mmseg - INFO - Iter [17200/20000]	lr: 8.403e-06, eta: 0:19:02, time: 0.405, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0451, decode.acc_seg: 50.8302, loss: 0.0451
2023-05-26 23:19:13,986 - mmseg - INFO - Iter [17250/20000]	lr: 8.253e-06, eta: 0:18:42, time: 0.452, data_time: 0.202, memory: 3387, decode.loss_ce: 0.0443, decode.acc_seg: 54.5763, loss: 0.0443
2023-05-26 23:19:34,104 - mmseg - INFO - Iter [17300/20000]	lr: 8.103e-06, eta: 0:18:21, time: 0.402, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0669, decode.acc_seg: 51.9484, loss: 0.0669
2023-05-26 23:19:54,324 - mmseg - INFO - Iter [17350/20000]	lr: 7.953e-06, eta: 0:18:01, time: 0.404, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0733, decode.acc_seg: 52.4436, loss: 0.0733
2023-05-26 23:20:14,677 - mmseg - INFO - Iter [17400/20000]	lr: 7.803e-06, eta: 0:17:41, time: 0.407, data_time: 0.154, memory: 3387, decode.loss_ce: 0.0478, decode.acc_seg: 52.3552, loss: 0.0478
2023-05-26 23:20:34,855 - mmseg - INFO - Iter [17450/20000]	lr: 7.653e-06, eta: 0:17:20, time: 0.404, data_time: 0.154, memory: 3387, decode.loss_ce: 0.0505, decode.acc_seg: 49.9921, loss: 0.0505
2023-05-26 23:20:54,359 - mmseg - INFO - Iter [17500/20000]	lr: 7.503e-06, eta: 0:17:00, time: 0.390, data_time: 0.154, memory: 3387, decode.loss_ce: 0.0455, decode.acc_seg: 52.8034, loss: 0.0455
2023-05-26 23:21:14,559 - mmseg - INFO - Iter [17550/20000]	lr: 7.353e-06, eta: 0:16:39, time: 0.404, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0448, decode.acc_seg: 51.8443, loss: 0.0448
2023-05-26 23:21:34,366 - mmseg - INFO - Iter [17600/20000]	lr: 7.203e-06, eta: 0:16:19, time: 0.396, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0434, decode.acc_seg: 53.1654, loss: 0.0434
2023-05-26 23:21:56,557 - mmseg - INFO - Iter [17650/20000]	lr: 7.053e-06, eta: 0:15:59, time: 0.444, data_time: 0.203, memory: 3387, decode.loss_ce: 0.0400, decode.acc_seg: 50.7261, loss: 0.0400
2023-05-26 23:22:16,743 - mmseg - INFO - Iter [17700/20000]	lr: 6.903e-06, eta: 0:15:38, time: 0.404, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0523, decode.acc_seg: 55.0365, loss: 0.0523
2023-05-26 23:22:36,910 - mmseg - INFO - Iter [17750/20000]	lr: 6.753e-06, eta: 0:15:18, time: 0.403, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0478, decode.acc_seg: 53.8533, loss: 0.0478
2023-05-26 23:22:56,951 - mmseg - INFO - Iter [17800/20000]	lr: 6.603e-06, eta: 0:14:57, time: 0.401, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0365, decode.acc_seg: 49.5660, loss: 0.0365
2023-05-26 23:23:16,997 - mmseg - INFO - Iter [17850/20000]	lr: 6.453e-06, eta: 0:14:37, time: 0.401, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0451, decode.acc_seg: 52.4178, loss: 0.0451
2023-05-26 23:23:37,046 - mmseg - INFO - Iter [17900/20000]	lr: 6.303e-06, eta: 0:14:16, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0409, decode.acc_seg: 54.0552, loss: 0.0409
2023-05-26 23:23:57,140 - mmseg - INFO - Iter [17950/20000]	lr: 6.153e-06, eta: 0:13:56, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0396, decode.acc_seg: 51.6682, loss: 0.0396
2023-05-26 23:24:17,370 - mmseg - INFO - Saving checkpoint at 18000 iterations
2023-05-26 23:24:19,514 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 23:24:19,514 - mmseg - INFO - Iter [18000/20000]	lr: 6.003e-06, eta: 0:13:36, time: 0.447, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0414, decode.acc_seg: 51.4166, loss: 0.0414
2023-05-26 23:24:22,095 - mmseg - INFO - per class results:
2023-05-26 23:24:22,096 - mmseg - INFO - 
+---------------+-------+-------+
|     Class     |  IoU  |  Acc  |
+---------------+-------+-------+
|   Dry Grass   |  0.0  |  0.0  |
|  Green Grass  | 95.42 | 99.26 |
|   Dry Shrubs  | 82.17 | 87.71 |
|  Green Shrubs |  54.6 | 55.77 |
|     Canopy    | 78.44 | 92.52 |
|  Wood Pieces  | 96.79 | 97.85 |
|   Litterfall  |  nan  |  nan  |
| Timber Litter | 96.72 | 98.99 |
|  Live Trunks  | 48.64 | 48.64 |
|   Bare Earth  | 96.72 | 96.72 |
|     People    | 97.39 | 97.58 |
|      Sky      |  nan  |  nan  |
|     Blurry    |  nan  |  nan  |
|   Obstacles   |  nan  |  nan  |
+---------------+-------+-------+
2023-05-26 23:24:22,096 - mmseg - INFO - Summary:
2023-05-26 23:24:22,096 - mmseg - INFO - 
+-------+-------+------+
|  aAcc |  mIoU | mAcc |
+-------+-------+------+
| 97.53 | 74.69 | 77.5 |
+-------+-------+------+
2023-05-26 23:24:22,097 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 23:24:22,097 - mmseg - INFO - Iter(val) [11]	aAcc: 0.9753, mIoU: 0.7469, mAcc: 0.7750, IoU.Dry Grass: 0.0000, IoU.Green Grass: 0.9542, IoU.Dry Shrubs: 0.8217, IoU.Green Shrubs: 0.5460, IoU.Canopy: 0.7844, IoU.Wood Pieces: 0.9679, IoU.Litterfall: nan, IoU.Timber Litter: 0.9672, IoU.Live Trunks: 0.4864, IoU.Bare Earth: 0.9672, IoU.People: 0.9739, IoU.Sky: nan, IoU.Blurry: nan, IoU.Obstacles: nan, Acc.Dry Grass: 0.0000, Acc.Green Grass: 0.9926, Acc.Dry Shrubs: 0.8771, Acc.Green Shrubs: 0.5577, Acc.Canopy: 0.9252, Acc.Wood Pieces: 0.9785, Acc.Litterfall: nan, Acc.Timber Litter: 0.9899, Acc.Live Trunks: 0.4864, Acc.Bare Earth: 0.9672, Acc.People: 0.9758, Acc.Sky: nan, Acc.Blurry: nan, Acc.Obstacles: nan
2023-05-26 23:24:44,411 - mmseg - INFO - Iter [18050/20000]	lr: 5.853e-06, eta: 0:13:16, time: 0.498, data_time: 0.253, memory: 3387, decode.loss_ce: 0.0449, decode.acc_seg: 51.2696, loss: 0.0449
2023-05-26 23:25:04,538 - mmseg - INFO - Iter [18100/20000]	lr: 5.703e-06, eta: 0:12:55, time: 0.403, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0403, decode.acc_seg: 52.4843, loss: 0.0403
2023-05-26 23:25:24,666 - mmseg - INFO - Iter [18150/20000]	lr: 5.553e-06, eta: 0:12:35, time: 0.403, data_time: 0.154, memory: 3387, decode.loss_ce: 0.0336, decode.acc_seg: 52.3346, loss: 0.0336
2023-05-26 23:25:44,599 - mmseg - INFO - Iter [18200/20000]	lr: 5.403e-06, eta: 0:12:14, time: 0.399, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0522, decode.acc_seg: 53.0779, loss: 0.0522
2023-05-26 23:26:04,640 - mmseg - INFO - Iter [18250/20000]	lr: 5.253e-06, eta: 0:11:54, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0369, decode.acc_seg: 50.7633, loss: 0.0369
2023-05-26 23:26:24,643 - mmseg - INFO - Iter [18300/20000]	lr: 5.103e-06, eta: 0:11:34, time: 0.400, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0399, decode.acc_seg: 51.1446, loss: 0.0399
2023-05-26 23:26:44,657 - mmseg - INFO - Iter [18350/20000]	lr: 4.953e-06, eta: 0:11:13, time: 0.400, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0493, decode.acc_seg: 52.0151, loss: 0.0493
2023-05-26 23:27:04,742 - mmseg - INFO - Iter [18400/20000]	lr: 4.803e-06, eta: 0:10:53, time: 0.402, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0461, decode.acc_seg: 50.0257, loss: 0.0461
2023-05-26 23:27:27,390 - mmseg - INFO - Iter [18450/20000]	lr: 4.653e-06, eta: 0:10:32, time: 0.453, data_time: 0.201, memory: 3387, decode.loss_ce: 0.0372, decode.acc_seg: 52.0371, loss: 0.0372
2023-05-26 23:27:47,040 - mmseg - INFO - Iter [18500/20000]	lr: 4.503e-06, eta: 0:10:12, time: 0.393, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0438, decode.acc_seg: 51.4678, loss: 0.0438
2023-05-26 23:28:07,100 - mmseg - INFO - Iter [18550/20000]	lr: 4.353e-06, eta: 0:09:52, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0426, decode.acc_seg: 51.6886, loss: 0.0426
2023-05-26 23:28:27,249 - mmseg - INFO - Iter [18600/20000]	lr: 4.203e-06, eta: 0:09:31, time: 0.403, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0347, decode.acc_seg: 53.1282, loss: 0.0347
2023-05-26 23:28:47,282 - mmseg - INFO - Iter [18650/20000]	lr: 4.053e-06, eta: 0:09:11, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0392, decode.acc_seg: 51.9983, loss: 0.0392
2023-05-26 23:29:07,308 - mmseg - INFO - Iter [18700/20000]	lr: 3.903e-06, eta: 0:08:50, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0367, decode.acc_seg: 53.4578, loss: 0.0367
2023-05-26 23:29:27,429 - mmseg - INFO - Iter [18750/20000]	lr: 3.753e-06, eta: 0:08:30, time: 0.402, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0355, decode.acc_seg: 51.1776, loss: 0.0355
2023-05-26 23:29:47,589 - mmseg - INFO - Iter [18800/20000]	lr: 3.603e-06, eta: 0:08:09, time: 0.403, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0412, decode.acc_seg: 52.5837, loss: 0.0412
2023-05-26 23:30:10,105 - mmseg - INFO - Iter [18850/20000]	lr: 3.453e-06, eta: 0:07:49, time: 0.450, data_time: 0.201, memory: 3387, decode.loss_ce: 0.0363, decode.acc_seg: 53.2309, loss: 0.0363
2023-05-26 23:30:30,233 - mmseg - INFO - Iter [18900/20000]	lr: 3.303e-06, eta: 0:07:29, time: 0.403, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0389, decode.acc_seg: 53.7863, loss: 0.0389
2023-05-26 23:30:50,242 - mmseg - INFO - Iter [18950/20000]	lr: 3.153e-06, eta: 0:07:08, time: 0.400, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0376, decode.acc_seg: 53.6538, loss: 0.0376
2023-05-26 23:31:10,272 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 23:31:10,273 - mmseg - INFO - Iter [19000/20000]	lr: 3.003e-06, eta: 0:06:48, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0466, decode.acc_seg: 51.9250, loss: 0.0466
2023-05-26 23:31:30,579 - mmseg - INFO - Iter [19050/20000]	lr: 2.853e-06, eta: 0:06:27, time: 0.406, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0301, decode.acc_seg: 51.7018, loss: 0.0301
2023-05-26 23:31:50,668 - mmseg - INFO - Iter [19100/20000]	lr: 2.703e-06, eta: 0:06:07, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0393, decode.acc_seg: 49.6095, loss: 0.0393
2023-05-26 23:32:10,831 - mmseg - INFO - Iter [19150/20000]	lr: 2.553e-06, eta: 0:05:46, time: 0.403, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0461, decode.acc_seg: 52.9014, loss: 0.0461
2023-05-26 23:32:30,831 - mmseg - INFO - Iter [19200/20000]	lr: 2.403e-06, eta: 0:05:26, time: 0.400, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0550, decode.acc_seg: 52.1915, loss: 0.0550
2023-05-26 23:32:53,349 - mmseg - INFO - Iter [19250/20000]	lr: 2.253e-06, eta: 0:05:06, time: 0.450, data_time: 0.201, memory: 3387, decode.loss_ce: 0.0394, decode.acc_seg: 52.9286, loss: 0.0394
2023-05-26 23:33:13,547 - mmseg - INFO - Iter [19300/20000]	lr: 2.103e-06, eta: 0:04:45, time: 0.404, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0348, decode.acc_seg: 52.5534, loss: 0.0348
2023-05-26 23:33:33,724 - mmseg - INFO - Iter [19350/20000]	lr: 1.953e-06, eta: 0:04:25, time: 0.404, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0396, decode.acc_seg: 53.0437, loss: 0.0396
2023-05-26 23:33:53,665 - mmseg - INFO - Iter [19400/20000]	lr: 1.803e-06, eta: 0:04:04, time: 0.399, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0311, decode.acc_seg: 51.1041, loss: 0.0311
2023-05-26 23:34:13,754 - mmseg - INFO - Iter [19450/20000]	lr: 1.653e-06, eta: 0:03:44, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0404, decode.acc_seg: 49.6047, loss: 0.0404
2023-05-26 23:34:33,804 - mmseg - INFO - Iter [19500/20000]	lr: 1.503e-06, eta: 0:03:24, time: 0.401, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0363, decode.acc_seg: 53.9373, loss: 0.0363
2023-05-26 23:34:53,923 - mmseg - INFO - Iter [19550/20000]	lr: 1.353e-06, eta: 0:03:03, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0397, decode.acc_seg: 51.6531, loss: 0.0397
2023-05-26 23:35:13,819 - mmseg - INFO - Iter [19600/20000]	lr: 1.203e-06, eta: 0:02:43, time: 0.398, data_time: 0.151, memory: 3387, decode.loss_ce: 0.0379, decode.acc_seg: 51.1434, loss: 0.0379
2023-05-26 23:35:35,335 - mmseg - INFO - Iter [19650/20000]	lr: 1.053e-06, eta: 0:02:22, time: 0.430, data_time: 0.201, memory: 3387, decode.loss_ce: 0.0424, decode.acc_seg: 53.0140, loss: 0.0424
2023-05-26 23:35:54,456 - mmseg - INFO - Iter [19700/20000]	lr: 9.030e-07, eta: 0:02:02, time: 0.382, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0341, decode.acc_seg: 50.9796, loss: 0.0341
2023-05-26 23:36:14,233 - mmseg - INFO - Iter [19750/20000]	lr: 7.530e-07, eta: 0:01:42, time: 0.396, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0380, decode.acc_seg: 55.0941, loss: 0.0380
2023-05-26 23:36:34,371 - mmseg - INFO - Iter [19800/20000]	lr: 6.030e-07, eta: 0:01:21, time: 0.403, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0419, decode.acc_seg: 52.2740, loss: 0.0419
2023-05-26 23:36:54,492 - mmseg - INFO - Iter [19850/20000]	lr: 4.530e-07, eta: 0:01:01, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0416, decode.acc_seg: 55.2930, loss: 0.0416
2023-05-26 23:37:14,605 - mmseg - INFO - Iter [19900/20000]	lr: 3.030e-07, eta: 0:00:40, time: 0.402, data_time: 0.152, memory: 3387, decode.loss_ce: 0.0338, decode.acc_seg: 50.8364, loss: 0.0338
2023-05-26 23:37:34,782 - mmseg - INFO - Iter [19950/20000]	lr: 1.530e-07, eta: 0:00:20, time: 0.404, data_time: 0.154, memory: 3387, decode.loss_ce: 0.0384, decode.acc_seg: 51.1329, loss: 0.0384
2023-05-26 23:37:54,116 - mmseg - INFO - Saving checkpoint at 20000 iterations
2023-05-26 23:37:56,538 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 23:37:56,538 - mmseg - INFO - Iter [20000/20000]	lr: 3.000e-09, eta: 0:00:00, time: 0.435, data_time: 0.153, memory: 3387, decode.loss_ce: 0.0331, decode.acc_seg: 53.8151, loss: 0.0331
2023-05-26 23:37:59,067 - mmseg - INFO - per class results:
2023-05-26 23:37:59,068 - mmseg - INFO - 
+---------------+-------+-------+
|     Class     |  IoU  |  Acc  |
+---------------+-------+-------+
|   Dry Grass   |  0.0  |  0.0  |
|  Green Grass  | 96.28 | 99.12 |
|   Dry Shrubs  | 85.12 | 89.22 |
|  Green Shrubs | 53.76 | 57.66 |
|     Canopy    |  80.4 | 84.23 |
|  Wood Pieces  | 96.98 | 98.53 |
|   Litterfall  |  nan  |  nan  |
| Timber Litter | 98.92 | 98.92 |
|  Live Trunks  |  70.9 | 71.38 |
|   Bare Earth  | 93.91 | 93.91 |
|     People    | 97.92 | 98.25 |
|      Sky      |  nan  |  nan  |
|     Blurry    |  nan  |  nan  |
|   Obstacles   |  nan  |  nan  |
+---------------+-------+-------+
2023-05-26 23:37:59,068 - mmseg - INFO - Summary:
2023-05-26 23:37:59,069 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 97.86 | 77.42 | 79.12 |
+-------+-------+-------+
2023-05-26 23:37:59,069 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy.py
2023-05-26 23:37:59,069 - mmseg - INFO - Iter(val) [11]	aAcc: 0.9786, mIoU: 0.7742, mAcc: 0.7912, IoU.Dry Grass: 0.0000, IoU.Green Grass: 0.9628, IoU.Dry Shrubs: 0.8512, IoU.Green Shrubs: 0.5376, IoU.Canopy: 0.8040, IoU.Wood Pieces: 0.9698, IoU.Litterfall: nan, IoU.Timber Litter: 0.9892, IoU.Live Trunks: 0.7090, IoU.Bare Earth: 0.9391, IoU.People: 0.9792, IoU.Sky: nan, IoU.Blurry: nan, IoU.Obstacles: nan, Acc.Dry Grass: 0.0000, Acc.Green Grass: 0.9912, Acc.Dry Shrubs: 0.8922, Acc.Green Shrubs: 0.5766, Acc.Canopy: 0.8423, Acc.Wood Pieces: 0.9853, Acc.Litterfall: nan, Acc.Timber Litter: 0.9892, Acc.Live Trunks: 0.7138, Acc.Bare Earth: 0.9391, Acc.People: 0.9825, Acc.Sky: nan, Acc.Blurry: nan, Acc.Obstacles: nan
