2023-05-27 00:00:06,300 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.16 (default, Mar  2 2023, 03:21:46) [GCC 11.2.0]
CUDA available: True
GPU 0: GRID A100X-20C
CUDA_HOME: None
GCC: gcc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0
PyTorch: 1.10.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.11.0
OpenCV: 4.7.0
MMCV: 1.4.4
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.3
MMSegmentation: 0.20.2+609cf9d
------------------------------------------------------------

2023-05-27 00:00:06,300 - mmseg - INFO - Distributed training: False
2023-05-27 00:00:07,178 - mmseg - INFO - Config:
norm_cfg = dict(type='SyncBN', requires_grad=True)
model = dict(
    type='EncoderDecoder',
    pretrained='pretrain/mit_b5.pth',
    backbone=dict(
        type='MixVisionTransformer',
        in_channels=3,
        embed_dims=64,
        num_stages=4,
        num_layers=[3, 6, 40, 3],
        num_heads=[1, 2, 5, 8],
        patch_sizes=[7, 3, 3, 3],
        sr_ratios=[8, 4, 2, 1],
        out_indices=(0, 1, 2, 3),
        mlp_ratio=4,
        qkv_bias=True,
        drop_rate=0.0,
        attn_drop_rate=0.0,
        drop_path_rate=0.1),
    decode_head=dict(
        type='SegformerHead',
        in_channels=[64, 128, 320, 512],
        in_index=[0, 1, 2, 3],
        channels=256,
        dropout_ratio=0.1,
        num_classes=4,
        norm_cfg=dict(type='SyncBN', requires_grad=True),
        align_corners=False,
        loss_decode=dict(
            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),
    train_cfg=dict(),
    test_cfg=dict(mode='whole'))
dataset_type = 'CustomDataset'
data_root = '/ofo-share/repos-david/data/mmseg-training/safeforest23_compressed'
classes = ('Fuel', 'Canopy', 'Background', 'Trunks', 'unknown')
img_norm_cfg = dict(
    mean=[92.26357559, 97.2787342, 92.08041962],
    std=[72.90837497, 74.14328133, 67.93619411],
    to_rgb=True)
crop_size = (256, 512)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(848, 480), ratio_range=(0.5, 2.0)),
    dict(type='RandomCrop', crop_size=(256, 512), cat_max_ratio=0.75),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(
        type='Normalize',
        mean=[92.26357559, 97.2787342, 92.08041962],
        std=[72.90837497, 74.14328133, 67.93619411],
        to_rgb=True),
    dict(type='Pad', size=(256, 512), pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(848, 480),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[92.26357559, 97.2787342, 92.08041962],
                std=[72.90837497, 74.14328133, 67.93619411],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='CustomDataset',
        data_root=
        '/ofo-share/repos-david/data/mmseg-training/safeforest23_compressed',
        img_dir='img_dir/train',
        ann_dir='ann_dir/train',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations'),
            dict(type='Resize', img_scale=(848, 480), ratio_range=(0.5, 2.0)),
            dict(type='RandomCrop', crop_size=(256, 512), cat_max_ratio=0.75),
            dict(type='RandomFlip', prob=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(
                type='Normalize',
                mean=[92.26357559, 97.2787342, 92.08041962],
                std=[72.90837497, 74.14328133, 67.93619411],
                to_rgb=True),
            dict(type='Pad', size=(256, 512), pad_val=0, seg_pad_val=255),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_semantic_seg'])
        ],
        img_suffix='_rgb.png',
        seg_map_suffix='_segmentation.png',
        classes=('Fuel', 'Canopy', 'Background', 'Trunks', 'unknown')),
    val=dict(
        type='CustomDataset',
        data_root=
        '/ofo-share/repos-david/data/mmseg-training/safeforest23_compressed',
        img_dir='img_dir/val',
        ann_dir='ann_dir/val',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(848, 480),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[92.26357559, 97.2787342, 92.08041962],
                        std=[72.90837497, 74.14328133, 67.93619411],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        img_suffix='_rgb.png',
        seg_map_suffix='_segmentation.png',
        classes=('Fuel', 'Canopy', 'Background', 'Trunks', 'unknown')),
    test=dict(
        type='CustomDataset',
        data_root=
        '/ofo-share/repos-david/data/mmseg-training/safeforest23_compressed',
        img_dir='img_dir/test',
        ann_dir='ann_dir/test',
        img_suffix='_rgb.png',
        seg_map_suffix='_segmentation.png',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(848, 480),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[92.26357559, 97.2787342, 92.08041962],
                        std=[72.90837497, 74.14328133, 67.93619411],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        classes=('Fuel', 'Canopy', 'Background', 'Trunks', 'unknown')))
log_config = dict(
    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = True
optimizer = dict(
    type='AdamW',
    lr=6e-05,
    betas=(0.9, 0.999),
    weight_decay=0.01,
    paramwise_cfg=dict(
        custom_keys=dict(
            pos_block=dict(decay_mult=0.0),
            norm=dict(decay_mult=0.0),
            head=dict(lr_mult=10.0))))
optimizer_config = dict()
lr_config = dict(
    policy='poly',
    warmup='linear',
    warmup_iters=1500,
    warmup_ratio=1e-06,
    power=1.0,
    min_lr=0.0,
    by_epoch=False)
runner = dict(type='IterBasedRunner', max_iters=20000)
checkpoint_config = dict(by_epoch=False, interval=2000)
evaluation = dict(interval=2000, metric='mIoU', pre_eval=True)
work_dir = './work_dirs/segformer_mit-b5_512x512_20k_safeforest_over_canopy_compressed'
gpu_ids = range(0, 1)

2023-05-27 00:00:07,179 - mmseg - INFO - Set random seed to 1278149267, deterministic: False
2023-05-27 00:00:08,017 - mmseg - INFO - initialize MixVisionTransformer with init_cfg {'type': 'Pretrained', 'checkpoint': 'pretrain/mit_b5.pth'}
2023-05-27 00:00:08,930 - mmseg - INFO - initialize SegformerHead with init_cfg {'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
Name of parameter - Initialization information

backbone.layers.0.0.projection.weight - torch.Size([64, 3, 7, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.0.projection.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.0.norm.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.0.norm.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.norm1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.norm1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.attn.attn.in_proj_weight - torch.Size([192, 64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.attn.attn.in_proj_bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.attn.attn.out_proj.weight - torch.Size([64, 64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.attn.attn.out_proj.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.attn.sr.weight - torch.Size([64, 64, 8, 8]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.attn.sr.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.attn.norm.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.attn.norm.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.norm2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.norm2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.ffn.layers.0.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.ffn.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.ffn.layers.1.weight - torch.Size([256, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.ffn.layers.4.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.0.ffn.layers.4.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.norm1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.norm1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.attn.attn.in_proj_weight - torch.Size([192, 64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.attn.attn.in_proj_bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.attn.attn.out_proj.weight - torch.Size([64, 64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.attn.attn.out_proj.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.attn.sr.weight - torch.Size([64, 64, 8, 8]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.attn.sr.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.attn.norm.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.attn.norm.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.norm2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.norm2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.ffn.layers.0.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.ffn.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.ffn.layers.1.weight - torch.Size([256, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.ffn.layers.4.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.1.ffn.layers.4.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.norm1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.norm1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.attn.attn.in_proj_weight - torch.Size([192, 64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.attn.attn.in_proj_bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.attn.attn.out_proj.weight - torch.Size([64, 64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.attn.attn.out_proj.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.attn.sr.weight - torch.Size([64, 64, 8, 8]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.attn.sr.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.attn.norm.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.attn.norm.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.norm2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.norm2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.ffn.layers.0.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.ffn.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.ffn.layers.1.weight - torch.Size([256, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.ffn.layers.4.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.1.2.ffn.layers.4.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.0.2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.0.projection.weight - torch.Size([128, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.0.projection.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.0.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.0.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.norm1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.norm1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.attn.attn.in_proj_weight - torch.Size([384, 128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.attn.attn.in_proj_bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.attn.attn.out_proj.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.attn.attn.out_proj.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.attn.sr.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.attn.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.attn.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.norm2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.norm2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.ffn.layers.0.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.ffn.layers.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.ffn.layers.1.weight - torch.Size([512, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.ffn.layers.4.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.0.ffn.layers.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.norm1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.norm1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.attn.attn.in_proj_weight - torch.Size([384, 128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.attn.attn.in_proj_bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.attn.attn.out_proj.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.attn.attn.out_proj.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.attn.sr.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.attn.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.attn.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.norm2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.norm2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.ffn.layers.0.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.ffn.layers.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.ffn.layers.1.weight - torch.Size([512, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.ffn.layers.4.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.1.ffn.layers.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.norm1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.norm1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.attn.attn.in_proj_weight - torch.Size([384, 128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.attn.attn.in_proj_bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.attn.attn.out_proj.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.attn.attn.out_proj.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.attn.sr.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.attn.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.attn.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.norm2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.norm2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.ffn.layers.0.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.ffn.layers.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.ffn.layers.1.weight - torch.Size([512, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.ffn.layers.4.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.2.ffn.layers.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.norm1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.norm1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.attn.attn.in_proj_weight - torch.Size([384, 128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.attn.attn.in_proj_bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.attn.attn.out_proj.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.attn.attn.out_proj.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.attn.sr.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.attn.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.attn.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.norm2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.norm2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.ffn.layers.0.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.ffn.layers.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.ffn.layers.1.weight - torch.Size([512, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.ffn.layers.4.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.3.ffn.layers.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.norm1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.norm1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.attn.attn.in_proj_weight - torch.Size([384, 128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.attn.attn.in_proj_bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.attn.attn.out_proj.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.attn.attn.out_proj.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.attn.sr.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.attn.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.attn.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.norm2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.norm2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.ffn.layers.0.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.ffn.layers.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.ffn.layers.1.weight - torch.Size([512, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.ffn.layers.4.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.4.ffn.layers.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.norm1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.norm1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.attn.attn.in_proj_weight - torch.Size([384, 128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.attn.attn.in_proj_bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.attn.attn.out_proj.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.attn.attn.out_proj.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.attn.sr.weight - torch.Size([128, 128, 4, 4]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.attn.sr.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.attn.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.attn.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.norm2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.norm2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.ffn.layers.0.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.ffn.layers.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.ffn.layers.1.weight - torch.Size([512, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.ffn.layers.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.ffn.layers.4.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.1.5.ffn.layers.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.1.2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.0.projection.weight - torch.Size([320, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.0.projection.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.0.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.0.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.0.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.1.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.2.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.3.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.4.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.5.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.6.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.7.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.8.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.9.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.10.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.11.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.12.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.13.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.14.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.15.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.16.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.17.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.18.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.19.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.20.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.21.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.22.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.23.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.24.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.25.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.26.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.27.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.28.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.29.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.30.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.31.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.32.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.33.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.34.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.35.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.36.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.37.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.38.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.norm1.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.norm1.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.attn.attn.in_proj_weight - torch.Size([960, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.attn.attn.in_proj_bias - torch.Size([960]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.attn.attn.out_proj.weight - torch.Size([320, 320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.attn.attn.out_proj.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.attn.sr.weight - torch.Size([320, 320, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.attn.sr.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.attn.norm.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.attn.norm.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.norm2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.norm2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.ffn.layers.0.weight - torch.Size([1280, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.ffn.layers.0.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.ffn.layers.1.weight - torch.Size([1280, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.ffn.layers.1.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.ffn.layers.4.weight - torch.Size([320, 1280, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.1.39.ffn.layers.4.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.2.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.2.2.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.0.projection.weight - torch.Size([512, 320, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.0.projection.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.0.norm.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.0.norm.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.attn.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.attn.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.attn.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.attn.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.ffn.layers.0.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.ffn.layers.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.ffn.layers.1.weight - torch.Size([2048, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.ffn.layers.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.ffn.layers.4.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.0.ffn.layers.4.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.attn.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.attn.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.attn.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.attn.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.ffn.layers.0.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.ffn.layers.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.ffn.layers.1.weight - torch.Size([2048, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.ffn.layers.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.ffn.layers.4.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.1.ffn.layers.4.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.attn.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.attn.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.attn.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.attn.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.ffn.layers.0.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.ffn.layers.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.ffn.layers.1.weight - torch.Size([2048, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.ffn.layers.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.ffn.layers.4.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.1.2.ffn.layers.4.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.layers.3.2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.conv_seg.weight - torch.Size([4, 256, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.conv_seg.bias - torch.Size([4]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.convs.0.conv.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.convs.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.convs.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.convs.1.conv.weight - torch.Size([256, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.convs.1.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.convs.1.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.convs.2.conv.weight - torch.Size([256, 320, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.convs.2.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.convs.2.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.convs.3.conv.weight - torch.Size([256, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.convs.3.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.convs.3.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.fusion_conv.conv.weight - torch.Size([256, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in ConvModule  

decode_head.fusion_conv.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.fusion_conv.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  
2023-05-27 00:00:08,947 - mmseg - INFO - EncoderDecoder(
  (backbone): MixVisionTransformer(
    (layers): ModuleList(
      (0): ModuleList(
        (0): PatchEmbed(
          (projection): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
          (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
        )
        (1): ModuleList(
          (0): TransformerEncoderLayer(
            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
              (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): TransformerEncoderLayer(
            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
              (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (2): TransformerEncoderLayer(
            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
              (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      )
      (1): ModuleList(
        (0): PatchEmbed(
          (projection): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        )
        (1): ModuleList(
          (0): TransformerEncoderLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): TransformerEncoderLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (2): TransformerEncoderLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (3): TransformerEncoderLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (4): TransformerEncoderLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (5): TransformerEncoderLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      )
      (2): ModuleList(
        (0): PatchEmbed(
          (projection): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        )
        (1): ModuleList(
          (0): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (2): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (3): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (4): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (5): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (6): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (7): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (8): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (9): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (10): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (11): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (12): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (13): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (14): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (15): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (16): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (17): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (18): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (19): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (20): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (21): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (22): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (23): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (24): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (25): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (26): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (27): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (28): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (29): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (30): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (31): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (32): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (33): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (34): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (35): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (36): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (37): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (38): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (39): TransformerEncoderLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      )
      (3): ModuleList(
        (0): PatchEmbed(
          (projection): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        )
        (1): ModuleList(
          (0): TransformerEncoderLayer(
            (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): TransformerEncoderLayer(
            (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (2): TransformerEncoderLayer(
            (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientMultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): DropPath()
            )
            (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))
                (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
                (2): GELU()
                (3): Dropout(p=0.0, inplace=False)
                (4): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
                (5): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  init_cfg={'type': 'Pretrained', 'checkpoint': 'pretrain/mit_b5.pth'}
  (decode_head): SegformerHead(
    input_transform=multiple_select, ignore_index=255, align_corners=False
    (loss_decode): CrossEntropyLoss()
    (conv_seg): Conv2d(256, 4, kernel_size=(1, 1), stride=(1, 1))
    (dropout): Dropout2d(p=0.1, inplace=False)
    (convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (1): ConvModule(
        (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (2): ConvModule(
        (conv): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (3): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
    )
    (fusion_conv): ConvModule(
      (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activate): ReLU(inplace=True)
    )
  )
  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
)
2023-05-27 00:00:08,964 - mmseg - INFO - Loaded 2 images
2023-05-27 00:00:18,090 - mmseg - INFO - Loaded 3 images
2023-05-27 00:00:18,091 - mmseg - INFO - Start running, host: exouser@dev-07-derek-9-image-david, work_dir: /ofo-share/repos-david/mmsegmentation_mine/work_dirs/segformer_mit-b5_512x512_20k_safeforest_over_canopy_compressed
2023-05-27 00:00:18,091 - mmseg - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2023-05-27 00:00:18,091 - mmseg - INFO - workflow: [('train', 1)], max: 20000 iters
2023-05-27 00:00:18,092 - mmseg - INFO - Checkpoints will be saved to /ofo-share/repos-david/mmsegmentation_mine/work_dirs/segformer_mit-b5_512x512_20k_safeforest_over_canopy_compressed by HardDiskBackend.
2023-05-27 00:02:35,393 - mmseg - INFO - Iter [50/20000]	lr: 1.955e-06, eta: 15:12:34, time: 2.745, data_time: 2.406, memory: 3384, decode.loss_ce: 0.4785, decode.acc_seg: 15.7330, loss: 0.4785
2023-05-27 00:04:50,514 - mmseg - INFO - Iter [100/20000]	lr: 3.940e-06, eta: 15:03:18, time: 2.702, data_time: 2.442, memory: 3384, decode.loss_ce: 0.4428, decode.acc_seg: 23.5981, loss: 0.4428
2023-05-27 00:07:05,631 - mmseg - INFO - Iter [150/20000]	lr: 5.916e-06, eta: 14:58:41, time: 2.702, data_time: 2.442, memory: 3384, decode.loss_ce: 0.3963, decode.acc_seg: 24.8000, loss: 0.3963
2023-05-27 00:09:20,701 - mmseg - INFO - Iter [200/20000]	lr: 7.881e-06, eta: 14:55:11, time: 2.701, data_time: 2.442, memory: 3384, decode.loss_ce: 0.3256, decode.acc_seg: 27.9269, loss: 0.3256
2023-05-27 00:11:35,732 - mmseg - INFO - Iter [250/20000]	lr: 9.836e-06, eta: 14:52:07, time: 2.701, data_time: 2.441, memory: 3384, decode.loss_ce: 0.2609, decode.acc_seg: 30.2738, loss: 0.2609
2023-05-27 00:13:50,791 - mmseg - INFO - Iter [300/20000]	lr: 1.178e-05, eta: 14:49:22, time: 2.701, data_time: 2.442, memory: 3384, decode.loss_ce: 0.1970, decode.acc_seg: 33.0456, loss: 0.1970
2023-05-27 00:16:05,819 - mmseg - INFO - Iter [350/20000]	lr: 1.372e-05, eta: 14:46:43, time: 2.701, data_time: 2.442, memory: 3384, decode.loss_ce: 0.1554, decode.acc_seg: 32.6654, loss: 0.1554
2023-05-27 00:18:20,830 - mmseg - INFO - Iter [400/20000]	lr: 1.564e-05, eta: 14:44:10, time: 2.700, data_time: 2.444, memory: 3384, decode.loss_ce: 0.1359, decode.acc_seg: 34.5232, loss: 0.1359
2023-05-27 00:20:35,786 - mmseg - INFO - Iter [450/20000]	lr: 1.756e-05, eta: 14:41:38, time: 2.699, data_time: 2.444, memory: 3384, decode.loss_ce: 0.1032, decode.acc_seg: 34.9955, loss: 0.1032
2023-05-27 00:22:50,489 - mmseg - INFO - Iter [500/20000]	lr: 1.946e-05, eta: 14:39:00, time: 2.694, data_time: 2.441, memory: 3384, decode.loss_ce: 0.1088, decode.acc_seg: 33.1199, loss: 0.1088
2023-05-27 00:25:05,977 - mmseg - INFO - Iter [550/20000]	lr: 2.136e-05, eta: 14:36:54, time: 2.710, data_time: 2.446, memory: 3384, decode.loss_ce: 0.0793, decode.acc_seg: 34.5410, loss: 0.0793
2023-05-27 00:27:21,517 - mmseg - INFO - Iter [600/20000]	lr: 2.324e-05, eta: 14:34:48, time: 2.711, data_time: 2.445, memory: 3384, decode.loss_ce: 0.0796, decode.acc_seg: 33.3552, loss: 0.0796
2023-05-27 00:29:36,959 - mmseg - INFO - Iter [650/20000]	lr: 2.512e-05, eta: 14:32:37, time: 2.709, data_time: 2.442, memory: 3384, decode.loss_ce: 0.0784, decode.acc_seg: 32.8794, loss: 0.0784
2023-05-27 00:31:52,383 - mmseg - INFO - Iter [700/20000]	lr: 2.698e-05, eta: 14:30:26, time: 2.708, data_time: 2.443, memory: 3384, decode.loss_ce: 0.0826, decode.acc_seg: 33.4586, loss: 0.0826
2023-05-27 00:34:07,852 - mmseg - INFO - Iter [750/20000]	lr: 2.884e-05, eta: 14:28:15, time: 2.709, data_time: 2.441, memory: 3384, decode.loss_ce: 0.0669, decode.acc_seg: 34.8395, loss: 0.0669
2023-05-27 00:36:23,376 - mmseg - INFO - Iter [800/20000]	lr: 3.068e-05, eta: 14:26:04, time: 2.710, data_time: 2.443, memory: 3384, decode.loss_ce: 0.0661, decode.acc_seg: 33.8665, loss: 0.0661
2023-05-27 00:38:38,841 - mmseg - INFO - Iter [850/20000]	lr: 3.252e-05, eta: 14:23:52, time: 2.709, data_time: 2.444, memory: 3384, decode.loss_ce: 0.0829, decode.acc_seg: 33.1461, loss: 0.0829
2023-05-27 00:40:54,375 - mmseg - INFO - Iter [900/20000]	lr: 3.434e-05, eta: 14:21:41, time: 2.711, data_time: 2.445, memory: 3384, decode.loss_ce: 0.0697, decode.acc_seg: 36.1977, loss: 0.0697
2023-05-27 00:43:10,046 - mmseg - INFO - Iter [950/20000]	lr: 3.616e-05, eta: 14:19:32, time: 2.713, data_time: 2.444, memory: 3384, decode.loss_ce: 0.0609, decode.acc_seg: 35.0454, loss: 0.0609
2023-05-27 00:45:25,498 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy_compressed.py
2023-05-27 00:45:25,505 - mmseg - INFO - Iter [1000/20000]	lr: 3.796e-05, eta: 14:17:19, time: 2.709, data_time: 2.444, memory: 3384, decode.loss_ce: 0.0677, decode.acc_seg: 34.5931, loss: 0.0677
2023-05-27 00:47:40,920 - mmseg - INFO - Iter [1050/20000]	lr: 3.976e-05, eta: 14:15:04, time: 2.708, data_time: 2.445, memory: 3384, decode.loss_ce: 0.0587, decode.acc_seg: 35.6100, loss: 0.0587
2023-05-27 00:49:56,348 - mmseg - INFO - Iter [1100/20000]	lr: 4.154e-05, eta: 14:12:50, time: 2.709, data_time: 2.447, memory: 3384, decode.loss_ce: 0.0474, decode.acc_seg: 36.4281, loss: 0.0474
2023-05-27 00:52:11,632 - mmseg - INFO - Iter [1150/20000]	lr: 4.332e-05, eta: 14:10:33, time: 2.706, data_time: 2.445, memory: 3384, decode.loss_ce: 0.0498, decode.acc_seg: 34.0897, loss: 0.0498
2023-05-27 00:54:27,064 - mmseg - INFO - Iter [1200/20000]	lr: 4.508e-05, eta: 14:08:19, time: 2.709, data_time: 2.447, memory: 3384, decode.loss_ce: 0.0631, decode.acc_seg: 35.8565, loss: 0.0631
2023-05-27 00:56:42,239 - mmseg - INFO - Iter [1250/20000]	lr: 4.684e-05, eta: 14:06:00, time: 2.703, data_time: 2.443, memory: 3384, decode.loss_ce: 0.0672, decode.acc_seg: 34.8902, loss: 0.0672
2023-05-27 00:58:57,627 - mmseg - INFO - Iter [1300/20000]	lr: 4.859e-05, eta: 14:03:45, time: 2.708, data_time: 2.445, memory: 3384, decode.loss_ce: 0.0506, decode.acc_seg: 35.6847, loss: 0.0506
2023-05-27 01:01:13,072 - mmseg - INFO - Iter [1350/20000]	lr: 5.032e-05, eta: 14:01:31, time: 2.709, data_time: 2.446, memory: 3384, decode.loss_ce: 0.0451, decode.acc_seg: 35.4150, loss: 0.0451
2023-05-27 01:03:28,309 - mmseg - INFO - Iter [1400/20000]	lr: 5.205e-05, eta: 13:59:14, time: 2.705, data_time: 2.443, memory: 3384, decode.loss_ce: 0.0443, decode.acc_seg: 34.9155, loss: 0.0443
2023-05-27 01:05:43,752 - mmseg - INFO - Iter [1450/20000]	lr: 5.376e-05, eta: 13:57:00, time: 2.709, data_time: 2.447, memory: 3384, decode.loss_ce: 0.0391, decode.acc_seg: 36.6434, loss: 0.0391
2023-05-27 01:07:59,021 - mmseg - INFO - Iter [1500/20000]	lr: 5.547e-05, eta: 13:54:43, time: 2.705, data_time: 2.444, memory: 3384, decode.loss_ce: 0.0458, decode.acc_seg: 36.1773, loss: 0.0458
2023-05-27 01:10:13,936 - mmseg - INFO - Iter [1550/20000]	lr: 5.535e-05, eta: 13:52:23, time: 2.698, data_time: 2.446, memory: 3384, decode.loss_ce: 0.0327, decode.acc_seg: 37.3425, loss: 0.0327
2023-05-27 01:12:29,101 - mmseg - INFO - Iter [1600/20000]	lr: 5.520e-05, eta: 13:50:05, time: 2.703, data_time: 2.444, memory: 3384, decode.loss_ce: 0.0378, decode.acc_seg: 36.0906, loss: 0.0378
2023-05-27 01:14:44,409 - mmseg - INFO - Iter [1650/20000]	lr: 5.505e-05, eta: 13:47:49, time: 2.706, data_time: 2.444, memory: 3384, decode.loss_ce: 0.0331, decode.acc_seg: 35.3038, loss: 0.0331
2023-05-27 01:16:59,986 - mmseg - INFO - Iter [1700/20000]	lr: 5.490e-05, eta: 13:45:37, time: 2.712, data_time: 2.448, memory: 3384, decode.loss_ce: 0.0249, decode.acc_seg: 35.6402, loss: 0.0249
2023-05-27 01:19:15,558 - mmseg - INFO - Iter [1750/20000]	lr: 5.475e-05, eta: 13:43:24, time: 2.711, data_time: 2.445, memory: 3384, decode.loss_ce: 0.0437, decode.acc_seg: 34.6386, loss: 0.0437
2023-05-27 01:21:30,855 - mmseg - INFO - Iter [1800/20000]	lr: 5.460e-05, eta: 13:41:08, time: 2.706, data_time: 2.446, memory: 3384, decode.loss_ce: 0.0282, decode.acc_seg: 36.8889, loss: 0.0282
2023-05-27 01:23:46,523 - mmseg - INFO - Iter [1850/20000]	lr: 5.445e-05, eta: 13:38:55, time: 2.713, data_time: 2.446, memory: 3384, decode.loss_ce: 0.0424, decode.acc_seg: 36.3100, loss: 0.0424
2023-05-27 01:26:01,877 - mmseg - INFO - Iter [1900/20000]	lr: 5.430e-05, eta: 13:36:40, time: 2.707, data_time: 2.447, memory: 3384, decode.loss_ce: 0.0263, decode.acc_seg: 36.9458, loss: 0.0263
2023-05-27 01:28:17,221 - mmseg - INFO - Iter [1950/20000]	lr: 5.415e-05, eta: 13:34:24, time: 2.707, data_time: 2.446, memory: 3384, decode.loss_ce: 0.0238, decode.acc_seg: 35.3466, loss: 0.0238
2023-05-27 01:30:32,381 - mmseg - INFO - Saving checkpoint at 2000 iterations
2023-05-27 01:30:34,737 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy_compressed.py
2023-05-27 01:30:34,737 - mmseg - INFO - Iter [2000/20000]	lr: 5.400e-05, eta: 13:32:28, time: 2.750, data_time: 2.443, memory: 3384, decode.loss_ce: 0.0288, decode.acc_seg: 36.7840, loss: 0.0288
2023-05-27 01:30:35,802 - mmseg - INFO - per class results:
2023-05-27 01:30:35,804 - mmseg - INFO - 
+------------+-------+-------+
|   Class    |  IoU  |  Acc  |
+------------+-------+-------+
|    Fuel    | 38.75 |  49.6 |
|   Canopy   | 49.46 | 99.87 |
| Background | 64.75 | 64.75 |
|   Trunks   |  nan  |  nan  |
|  unknown   |  nan  |  nan  |
+------------+-------+-------+
2023-05-27 01:30:35,804 - mmseg - INFO - Summary:
2023-05-27 01:30:35,805 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 65.72 | 50.99 | 71.41 |
+-------+-------+-------+
2023-05-27 01:30:35,806 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy_compressed.py
2023-05-27 01:30:35,806 - mmseg - INFO - Iter(val) [3]	aAcc: 0.6572, mIoU: 0.5099, mAcc: 0.7141, IoU.Fuel: 0.3875, IoU.Canopy: 0.4946, IoU.Background: 0.6475, IoU.Trunks: nan, IoU.unknown: nan, Acc.Fuel: 0.4960, Acc.Canopy: 0.9987, Acc.Background: 0.6475, Acc.Trunks: nan, Acc.unknown: nan
2023-05-27 01:32:51,195 - mmseg - INFO - Iter [2050/20000]	lr: 5.385e-05, eta: 13:30:22, time: 2.729, data_time: 2.467, memory: 3386, decode.loss_ce: 0.0187, decode.acc_seg: 36.0844, loss: 0.0187
2023-05-27 01:35:06,294 - mmseg - INFO - Iter [2100/20000]	lr: 5.370e-05, eta: 13:28:04, time: 2.702, data_time: 2.444, memory: 3386, decode.loss_ce: 0.0219, decode.acc_seg: 35.8359, loss: 0.0219
2023-05-27 01:37:21,314 - mmseg - INFO - Iter [2150/20000]	lr: 5.355e-05, eta: 13:25:45, time: 2.700, data_time: 2.443, memory: 3386, decode.loss_ce: 0.0194, decode.acc_seg: 34.2453, loss: 0.0194
2023-05-27 01:39:36,440 - mmseg - INFO - Iter [2200/20000]	lr: 5.340e-05, eta: 13:23:27, time: 2.703, data_time: 2.444, memory: 3386, decode.loss_ce: 0.0181, decode.acc_seg: 36.7422, loss: 0.0181
2023-05-27 01:41:51,717 - mmseg - INFO - Iter [2250/20000]	lr: 5.325e-05, eta: 13:21:11, time: 2.706, data_time: 2.444, memory: 3386, decode.loss_ce: 0.0251, decode.acc_seg: 35.7133, loss: 0.0251
2023-05-27 01:44:07,285 - mmseg - INFO - Iter [2300/20000]	lr: 5.310e-05, eta: 13:18:56, time: 2.711, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0177, decode.acc_seg: 37.1913, loss: 0.0177
2023-05-27 01:46:22,294 - mmseg - INFO - Iter [2350/20000]	lr: 5.295e-05, eta: 13:16:38, time: 2.700, data_time: 2.444, memory: 3386, decode.loss_ce: 0.0165, decode.acc_seg: 35.3168, loss: 0.0165
2023-05-27 01:48:36,860 - mmseg - INFO - Iter [2400/20000]	lr: 5.280e-05, eta: 13:14:16, time: 2.691, data_time: 2.442, memory: 3386, decode.loss_ce: 0.0239, decode.acc_seg: 37.3665, loss: 0.0239
2023-05-27 01:50:51,624 - mmseg - INFO - Iter [2450/20000]	lr: 5.265e-05, eta: 13:11:57, time: 2.695, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0185, decode.acc_seg: 36.7925, loss: 0.0185
2023-05-27 01:53:06,350 - mmseg - INFO - Iter [2500/20000]	lr: 5.250e-05, eta: 13:09:37, time: 2.694, data_time: 2.446, memory: 3386, decode.loss_ce: 0.0331, decode.acc_seg: 35.6158, loss: 0.0331
2023-05-27 01:55:21,661 - mmseg - INFO - Iter [2550/20000]	lr: 5.235e-05, eta: 13:07:21, time: 2.706, data_time: 2.444, memory: 3386, decode.loss_ce: 0.0278, decode.acc_seg: 35.4182, loss: 0.0278
2023-05-27 01:57:37,031 - mmseg - INFO - Iter [2600/20000]	lr: 5.220e-05, eta: 13:05:06, time: 2.707, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0153, decode.acc_seg: 37.8270, loss: 0.0153
2023-05-27 01:59:52,423 - mmseg - INFO - Iter [2650/20000]	lr: 5.205e-05, eta: 13:02:50, time: 2.708, data_time: 2.446, memory: 3386, decode.loss_ce: 0.0226, decode.acc_seg: 35.4648, loss: 0.0226
2023-05-27 02:02:07,702 - mmseg - INFO - Iter [2700/20000]	lr: 5.190e-05, eta: 13:00:35, time: 2.706, data_time: 2.447, memory: 3386, decode.loss_ce: 0.0197, decode.acc_seg: 35.5199, loss: 0.0197
2023-05-27 02:04:22,982 - mmseg - INFO - Iter [2750/20000]	lr: 5.175e-05, eta: 12:58:19, time: 2.706, data_time: 2.446, memory: 3386, decode.loss_ce: 0.0166, decode.acc_seg: 35.4928, loss: 0.0166
2023-05-27 02:06:38,501 - mmseg - INFO - Iter [2800/20000]	lr: 5.160e-05, eta: 12:56:04, time: 2.710, data_time: 2.450, memory: 3386, decode.loss_ce: 0.0134, decode.acc_seg: 36.3435, loss: 0.0134
2023-05-27 02:08:53,757 - mmseg - INFO - Iter [2850/20000]	lr: 5.145e-05, eta: 12:53:48, time: 2.705, data_time: 2.446, memory: 3386, decode.loss_ce: 0.0088, decode.acc_seg: 35.8323, loss: 0.0088
2023-05-27 02:11:09,079 - mmseg - INFO - Iter [2900/20000]	lr: 5.130e-05, eta: 12:51:33, time: 2.706, data_time: 2.448, memory: 3386, decode.loss_ce: 0.0134, decode.acc_seg: 35.4600, loss: 0.0134
2023-05-27 02:13:24,276 - mmseg - INFO - Iter [2950/20000]	lr: 5.115e-05, eta: 12:49:16, time: 2.704, data_time: 2.446, memory: 3386, decode.loss_ce: 0.0111, decode.acc_seg: 36.5788, loss: 0.0111
2023-05-27 02:15:39,522 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy_compressed.py
2023-05-27 02:15:39,526 - mmseg - INFO - Iter [3000/20000]	lr: 5.100e-05, eta: 12:47:00, time: 2.705, data_time: 2.447, memory: 3386, decode.loss_ce: 0.0102, decode.acc_seg: 36.7047, loss: 0.0102
2023-05-27 02:17:54,735 - mmseg - INFO - Iter [3050/20000]	lr: 5.085e-05, eta: 12:44:44, time: 2.704, data_time: 2.446, memory: 3386, decode.loss_ce: 0.0080, decode.acc_seg: 35.6819, loss: 0.0080
2023-05-27 02:20:10,120 - mmseg - INFO - Iter [3100/20000]	lr: 5.070e-05, eta: 12:42:29, time: 2.708, data_time: 2.444, memory: 3386, decode.loss_ce: 0.0099, decode.acc_seg: 36.6386, loss: 0.0099
2023-05-27 02:22:25,426 - mmseg - INFO - Iter [3150/20000]	lr: 5.055e-05, eta: 12:40:13, time: 2.706, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0135, decode.acc_seg: 38.2932, loss: 0.0135
2023-05-27 02:24:40,666 - mmseg - INFO - Iter [3200/20000]	lr: 5.040e-05, eta: 12:37:57, time: 2.705, data_time: 2.442, memory: 3386, decode.loss_ce: 0.0074, decode.acc_seg: 37.8726, loss: 0.0074
2023-05-27 02:26:55,878 - mmseg - INFO - Iter [3250/20000]	lr: 5.025e-05, eta: 12:35:41, time: 2.704, data_time: 2.441, memory: 3386, decode.loss_ce: 0.0075, decode.acc_seg: 36.0681, loss: 0.0075
2023-05-27 02:29:10,949 - mmseg - INFO - Iter [3300/20000]	lr: 5.010e-05, eta: 12:33:25, time: 2.701, data_time: 2.441, memory: 3386, decode.loss_ce: 0.0068, decode.acc_seg: 35.2104, loss: 0.0068
2023-05-27 02:31:26,058 - mmseg - INFO - Iter [3350/20000]	lr: 4.995e-05, eta: 12:31:08, time: 2.702, data_time: 2.441, memory: 3386, decode.loss_ce: 0.0104, decode.acc_seg: 35.6456, loss: 0.0104
2023-05-27 02:33:41,274 - mmseg - INFO - Iter [3400/20000]	lr: 4.980e-05, eta: 12:28:52, time: 2.704, data_time: 2.443, memory: 3386, decode.loss_ce: 0.0058, decode.acc_seg: 36.8463, loss: 0.0058
2023-05-27 02:35:56,692 - mmseg - INFO - Iter [3450/20000]	lr: 4.965e-05, eta: 12:26:37, time: 2.708, data_time: 2.448, memory: 3386, decode.loss_ce: 0.0065, decode.acc_seg: 38.6149, loss: 0.0065
2023-05-27 02:38:12,026 - mmseg - INFO - Iter [3500/20000]	lr: 4.950e-05, eta: 12:24:22, time: 2.707, data_time: 2.446, memory: 3386, decode.loss_ce: 0.0045, decode.acc_seg: 36.7365, loss: 0.0045
2023-05-27 02:40:27,174 - mmseg - INFO - Iter [3550/20000]	lr: 4.935e-05, eta: 12:22:06, time: 2.703, data_time: 2.444, memory: 3386, decode.loss_ce: 0.0073, decode.acc_seg: 35.4545, loss: 0.0073
2023-05-27 02:42:42,294 - mmseg - INFO - Iter [3600/20000]	lr: 4.920e-05, eta: 12:19:49, time: 2.702, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0085, decode.acc_seg: 36.2572, loss: 0.0085
2023-05-27 02:44:57,317 - mmseg - INFO - Iter [3650/20000]	lr: 4.905e-05, eta: 12:17:33, time: 2.700, data_time: 2.442, memory: 3386, decode.loss_ce: 0.0081, decode.acc_seg: 34.0889, loss: 0.0081
2023-05-27 02:47:12,515 - mmseg - INFO - Iter [3700/20000]	lr: 4.890e-05, eta: 12:15:17, time: 2.704, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0064, decode.acc_seg: 37.6547, loss: 0.0064
2023-05-27 02:49:27,790 - mmseg - INFO - Iter [3750/20000]	lr: 4.875e-05, eta: 12:13:01, time: 2.706, data_time: 2.446, memory: 3386, decode.loss_ce: 0.0049, decode.acc_seg: 35.9291, loss: 0.0049
2023-05-27 02:51:43,134 - mmseg - INFO - Iter [3800/20000]	lr: 4.860e-05, eta: 12:10:46, time: 2.707, data_time: 2.446, memory: 3386, decode.loss_ce: 0.0031, decode.acc_seg: 37.3720, loss: 0.0031
2023-05-27 02:53:58,441 - mmseg - INFO - Iter [3850/20000]	lr: 4.845e-05, eta: 12:08:30, time: 2.706, data_time: 2.447, memory: 3386, decode.loss_ce: 0.0032, decode.acc_seg: 36.8381, loss: 0.0032
2023-05-27 02:56:14,043 - mmseg - INFO - Iter [3900/20000]	lr: 4.830e-05, eta: 12:06:16, time: 2.712, data_time: 2.449, memory: 3386, decode.loss_ce: 0.0027, decode.acc_seg: 38.2675, loss: 0.0027
2023-05-27 02:58:29,345 - mmseg - INFO - Iter [3950/20000]	lr: 4.815e-05, eta: 12:04:01, time: 2.706, data_time: 2.446, memory: 3386, decode.loss_ce: 0.0044, decode.acc_seg: 37.8200, loss: 0.0044
2023-05-27 03:00:44,534 - mmseg - INFO - Saving checkpoint at 4000 iterations
2023-05-27 03:00:46,927 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy_compressed.py
2023-05-27 03:00:46,927 - mmseg - INFO - Iter [4000/20000]	lr: 4.800e-05, eta: 12:01:54, time: 2.752, data_time: 2.444, memory: 3386, decode.loss_ce: 0.0051, decode.acc_seg: 36.5709, loss: 0.0051
2023-05-27 03:00:47,736 - mmseg - INFO - per class results:
2023-05-27 03:00:47,737 - mmseg - INFO - 
+------------+-------+-------+
|   Class    |  IoU  |  Acc  |
+------------+-------+-------+
|    Fuel    | 59.41 | 64.34 |
|   Canopy   | 57.18 |  95.9 |
| Background | 89.28 | 91.89 |
|   Trunks   |  nan  |  nan  |
|  unknown   |  nan  |  nan  |
+------------+-------+-------+
2023-05-27 03:00:47,737 - mmseg - INFO - Summary:
2023-05-27 03:00:47,737 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 80.71 | 68.62 | 84.04 |
+-------+-------+-------+
2023-05-27 03:00:47,738 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy_compressed.py
2023-05-27 03:00:47,738 - mmseg - INFO - Iter(val) [3]	aAcc: 0.8071, mIoU: 0.6862, mAcc: 0.8404, IoU.Fuel: 0.5941, IoU.Canopy: 0.5718, IoU.Background: 0.8928, IoU.Trunks: nan, IoU.unknown: nan, Acc.Fuel: 0.6434, Acc.Canopy: 0.9590, Acc.Background: 0.9189, Acc.Trunks: nan, Acc.unknown: nan
2023-05-27 03:03:02,962 - mmseg - INFO - Iter [4050/20000]	lr: 4.785e-05, eta: 11:59:42, time: 2.721, data_time: 2.460, memory: 3386, decode.loss_ce: 0.0057, decode.acc_seg: 38.5635, loss: 0.0057
2023-05-27 03:05:18,355 - mmseg - INFO - Iter [4100/20000]	lr: 4.770e-05, eta: 11:57:26, time: 2.708, data_time: 2.447, memory: 3386, decode.loss_ce: 0.0040, decode.acc_seg: 37.1375, loss: 0.0040
2023-05-27 03:07:33,668 - mmseg - INFO - Iter [4150/20000]	lr: 4.755e-05, eta: 11:55:11, time: 2.706, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0397, decode.acc_seg: 34.9863, loss: 0.0397
2023-05-27 03:09:48,949 - mmseg - INFO - Iter [4200/20000]	lr: 4.740e-05, eta: 11:52:55, time: 2.706, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0249, decode.acc_seg: 36.8806, loss: 0.0249
2023-05-27 03:12:04,169 - mmseg - INFO - Iter [4250/20000]	lr: 4.725e-05, eta: 11:50:39, time: 2.704, data_time: 2.444, memory: 3386, decode.loss_ce: 0.0149, decode.acc_seg: 37.1303, loss: 0.0149
2023-05-27 03:14:19,373 - mmseg - INFO - Iter [4300/20000]	lr: 4.710e-05, eta: 11:48:23, time: 2.704, data_time: 2.444, memory: 3386, decode.loss_ce: 0.0113, decode.acc_seg: 35.7809, loss: 0.0113
2023-05-27 03:16:34,588 - mmseg - INFO - Iter [4350/20000]	lr: 4.695e-05, eta: 11:46:07, time: 2.704, data_time: 2.443, memory: 3386, decode.loss_ce: 0.0131, decode.acc_seg: 36.1350, loss: 0.0131
2023-05-27 03:18:49,888 - mmseg - INFO - Iter [4400/20000]	lr: 4.680e-05, eta: 11:43:52, time: 2.706, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0389, decode.acc_seg: 36.6020, loss: 0.0389
2023-05-27 03:21:05,216 - mmseg - INFO - Iter [4450/20000]	lr: 4.665e-05, eta: 11:41:36, time: 2.707, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0128, decode.acc_seg: 38.4847, loss: 0.0128
2023-05-27 03:23:20,473 - mmseg - INFO - Iter [4500/20000]	lr: 4.650e-05, eta: 11:39:21, time: 2.705, data_time: 2.444, memory: 3386, decode.loss_ce: 0.0239, decode.acc_seg: 36.2805, loss: 0.0239
2023-05-27 03:25:36,038 - mmseg - INFO - Iter [4550/20000]	lr: 4.635e-05, eta: 11:37:06, time: 2.711, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0129, decode.acc_seg: 36.8086, loss: 0.0129
2023-05-27 03:27:51,625 - mmseg - INFO - Iter [4600/20000]	lr: 4.620e-05, eta: 11:34:51, time: 2.712, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0204, decode.acc_seg: 36.6907, loss: 0.0204
2023-05-27 03:30:06,981 - mmseg - INFO - Iter [4650/20000]	lr: 4.605e-05, eta: 11:32:36, time: 2.707, data_time: 2.443, memory: 3386, decode.loss_ce: 0.0117, decode.acc_seg: 36.4039, loss: 0.0117
2023-05-27 03:32:22,428 - mmseg - INFO - Iter [4700/20000]	lr: 4.590e-05, eta: 11:30:21, time: 2.709, data_time: 2.444, memory: 3386, decode.loss_ce: 0.0066, decode.acc_seg: 36.2146, loss: 0.0066
2023-05-27 03:34:37,870 - mmseg - INFO - Iter [4750/20000]	lr: 4.575e-05, eta: 11:28:06, time: 2.709, data_time: 2.443, memory: 3386, decode.loss_ce: 0.0061, decode.acc_seg: 37.8111, loss: 0.0061
2023-05-27 03:36:53,425 - mmseg - INFO - Iter [4800/20000]	lr: 4.560e-05, eta: 11:25:51, time: 2.711, data_time: 2.444, memory: 3386, decode.loss_ce: 0.0047, decode.acc_seg: 36.9552, loss: 0.0047
2023-05-27 03:39:09,006 - mmseg - INFO - Iter [4850/20000]	lr: 4.545e-05, eta: 11:23:36, time: 2.712, data_time: 2.444, memory: 3386, decode.loss_ce: 0.0077, decode.acc_seg: 36.5314, loss: 0.0077
2023-05-27 03:41:24,255 - mmseg - INFO - Iter [4900/20000]	lr: 4.530e-05, eta: 11:21:21, time: 2.705, data_time: 2.440, memory: 3386, decode.loss_ce: 0.0051, decode.acc_seg: 36.7501, loss: 0.0051
2023-05-27 03:43:38,739 - mmseg - INFO - Iter [4950/20000]	lr: 4.515e-05, eta: 11:19:03, time: 2.690, data_time: 2.440, memory: 3386, decode.loss_ce: 0.0045, decode.acc_seg: 36.8197, loss: 0.0045
2023-05-27 03:45:53,381 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy_compressed.py
2023-05-27 03:45:53,384 - mmseg - INFO - Iter [5000/20000]	lr: 4.500e-05, eta: 11:16:45, time: 2.693, data_time: 2.438, memory: 3386, decode.loss_ce: 0.0036, decode.acc_seg: 35.6254, loss: 0.0036
2023-05-27 03:48:07,924 - mmseg - INFO - Iter [5050/20000]	lr: 4.485e-05, eta: 11:14:27, time: 2.691, data_time: 2.441, memory: 3386, decode.loss_ce: 0.0042, decode.acc_seg: 37.4277, loss: 0.0042
2023-05-27 03:50:22,652 - mmseg - INFO - Iter [5100/20000]	lr: 4.470e-05, eta: 11:12:10, time: 2.695, data_time: 2.447, memory: 3386, decode.loss_ce: 0.0044, decode.acc_seg: 36.8998, loss: 0.0044
2023-05-27 03:52:36,452 - mmseg - INFO - Iter [5150/20000]	lr: 4.455e-05, eta: 11:09:50, time: 2.676, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0029, decode.acc_seg: 37.8874, loss: 0.0029
2023-05-27 03:54:51,851 - mmseg - INFO - Iter [5200/20000]	lr: 4.440e-05, eta: 11:07:35, time: 2.708, data_time: 2.446, memory: 3386, decode.loss_ce: 0.0032, decode.acc_seg: 36.7671, loss: 0.0032
2023-05-27 03:57:07,184 - mmseg - INFO - Iter [5250/20000]	lr: 4.425e-05, eta: 11:05:20, time: 2.707, data_time: 2.444, memory: 3386, decode.loss_ce: 0.0025, decode.acc_seg: 36.3615, loss: 0.0025
2023-05-27 03:59:22,702 - mmseg - INFO - Iter [5300/20000]	lr: 4.410e-05, eta: 11:03:05, time: 2.710, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0023, decode.acc_seg: 36.4902, loss: 0.0023
2023-05-27 04:01:38,363 - mmseg - INFO - Iter [5350/20000]	lr: 4.395e-05, eta: 11:00:51, time: 2.713, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0044, decode.acc_seg: 36.2623, loss: 0.0044
2023-05-27 04:03:54,002 - mmseg - INFO - Iter [5400/20000]	lr: 4.380e-05, eta: 10:58:36, time: 2.713, data_time: 2.446, memory: 3386, decode.loss_ce: 0.0022, decode.acc_seg: 36.7028, loss: 0.0022
2023-05-27 04:06:09,203 - mmseg - INFO - Iter [5450/20000]	lr: 4.365e-05, eta: 10:56:21, time: 2.704, data_time: 2.447, memory: 3386, decode.loss_ce: 0.0396, decode.acc_seg: 37.5944, loss: 0.0396
2023-05-27 04:08:24,824 - mmseg - INFO - Iter [5500/20000]	lr: 4.350e-05, eta: 10:54:06, time: 2.712, data_time: 2.447, memory: 3386, decode.loss_ce: 0.0166, decode.acc_seg: 36.0214, loss: 0.0166
2023-05-27 04:10:40,157 - mmseg - INFO - Iter [5550/20000]	lr: 4.335e-05, eta: 10:51:51, time: 2.707, data_time: 2.443, memory: 3386, decode.loss_ce: 0.0072, decode.acc_seg: 36.9534, loss: 0.0072
2023-05-27 04:12:55,762 - mmseg - INFO - Iter [5600/20000]	lr: 4.320e-05, eta: 10:49:36, time: 2.712, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0066, decode.acc_seg: 37.5310, loss: 0.0066
2023-05-27 04:15:11,192 - mmseg - INFO - Iter [5650/20000]	lr: 4.305e-05, eta: 10:47:21, time: 2.709, data_time: 2.444, memory: 3386, decode.loss_ce: 0.0049, decode.acc_seg: 36.9343, loss: 0.0049
2023-05-27 04:17:26,632 - mmseg - INFO - Iter [5700/20000]	lr: 4.290e-05, eta: 10:45:06, time: 2.709, data_time: 2.442, memory: 3386, decode.loss_ce: 0.0043, decode.acc_seg: 37.2770, loss: 0.0043
2023-05-27 04:19:42,131 - mmseg - INFO - Iter [5750/20000]	lr: 4.275e-05, eta: 10:42:51, time: 2.710, data_time: 2.442, memory: 3386, decode.loss_ce: 0.0031, decode.acc_seg: 36.5286, loss: 0.0031
2023-05-27 04:21:56,679 - mmseg - INFO - Iter [5800/20000]	lr: 4.260e-05, eta: 10:40:34, time: 2.691, data_time: 2.444, memory: 3386, decode.loss_ce: 0.0026, decode.acc_seg: 36.7585, loss: 0.0026
2023-05-27 04:24:12,535 - mmseg - INFO - Iter [5850/20000]	lr: 4.245e-05, eta: 10:38:20, time: 2.717, data_time: 2.452, memory: 3386, decode.loss_ce: 0.0036, decode.acc_seg: 36.3346, loss: 0.0036
2023-05-27 04:26:28,003 - mmseg - INFO - Iter [5900/20000]	lr: 4.230e-05, eta: 10:36:05, time: 2.709, data_time: 2.446, memory: 3386, decode.loss_ce: 0.0064, decode.acc_seg: 38.1374, loss: 0.0064
2023-05-27 04:28:43,311 - mmseg - INFO - Iter [5950/20000]	lr: 4.215e-05, eta: 10:33:49, time: 2.706, data_time: 2.447, memory: 3386, decode.loss_ce: 0.0031, decode.acc_seg: 37.9138, loss: 0.0031
2023-05-27 04:30:58,355 - mmseg - INFO - Saving checkpoint at 6000 iterations
2023-05-27 04:31:00,404 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy_compressed.py
2023-05-27 04:31:00,404 - mmseg - INFO - Iter [6000/20000]	lr: 4.200e-05, eta: 10:31:38, time: 2.742, data_time: 2.444, memory: 3386, decode.loss_ce: 0.0024, decode.acc_seg: 36.8617, loss: 0.0024
2023-05-27 04:31:01,124 - mmseg - INFO - per class results:
2023-05-27 04:31:01,125 - mmseg - INFO - 
+------------+-------+-------+
|   Class    |  IoU  |  Acc  |
+------------+-------+-------+
|    Fuel    | 33.33 | 34.45 |
|   Canopy   | 42.56 | 99.82 |
| Background | 93.15 | 93.87 |
|   Trunks   |  nan  |  nan  |
|  unknown   |  nan  |  nan  |
+------------+-------+-------+
2023-05-27 04:31:01,125 - mmseg - INFO - Summary:
2023-05-27 04:31:01,126 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 69.18 | 56.35 | 76.05 |
+-------+-------+-------+
2023-05-27 04:31:01,126 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy_compressed.py
2023-05-27 04:31:01,127 - mmseg - INFO - Iter(val) [3]	aAcc: 0.6918, mIoU: 0.5635, mAcc: 0.7605, IoU.Fuel: 0.3333, IoU.Canopy: 0.4256, IoU.Background: 0.9315, IoU.Trunks: nan, IoU.unknown: nan, Acc.Fuel: 0.3445, Acc.Canopy: 0.9982, Acc.Background: 0.9387, Acc.Trunks: nan, Acc.unknown: nan
2023-05-27 04:33:16,084 - mmseg - INFO - Iter [6050/20000]	lr: 4.185e-05, eta: 10:29:23, time: 2.714, data_time: 2.459, memory: 3386, decode.loss_ce: 0.0032, decode.acc_seg: 37.2205, loss: 0.0032
2023-05-27 04:35:31,538 - mmseg - INFO - Iter [6100/20000]	lr: 4.170e-05, eta: 10:27:08, time: 2.709, data_time: 2.453, memory: 3386, decode.loss_ce: 0.0025, decode.acc_seg: 37.4792, loss: 0.0025
2023-05-27 04:37:47,209 - mmseg - INFO - Iter [6150/20000]	lr: 4.155e-05, eta: 10:24:54, time: 2.713, data_time: 2.456, memory: 3386, decode.loss_ce: 0.0030, decode.acc_seg: 37.0247, loss: 0.0030
2023-05-27 04:40:02,551 - mmseg - INFO - Iter [6200/20000]	lr: 4.140e-05, eta: 10:22:38, time: 2.707, data_time: 2.447, memory: 3386, decode.loss_ce: 0.0033, decode.acc_seg: 35.7311, loss: 0.0033
2023-05-27 04:42:17,895 - mmseg - INFO - Iter [6250/20000]	lr: 4.125e-05, eta: 10:20:23, time: 2.707, data_time: 2.444, memory: 3386, decode.loss_ce: 0.0028, decode.acc_seg: 37.1160, loss: 0.0028
2023-05-27 04:44:33,127 - mmseg - INFO - Iter [6300/20000]	lr: 4.110e-05, eta: 10:18:07, time: 2.705, data_time: 2.443, memory: 3386, decode.loss_ce: 0.0019, decode.acc_seg: 39.4978, loss: 0.0019
2023-05-27 04:46:48,382 - mmseg - INFO - Iter [6350/20000]	lr: 4.095e-05, eta: 10:15:52, time: 2.705, data_time: 2.444, memory: 3386, decode.loss_ce: 0.0018, decode.acc_seg: 36.7228, loss: 0.0018
2023-05-27 04:49:03,695 - mmseg - INFO - Iter [6400/20000]	lr: 4.080e-05, eta: 10:13:36, time: 2.706, data_time: 2.444, memory: 3386, decode.loss_ce: 0.0025, decode.acc_seg: 35.4387, loss: 0.0025
2023-05-27 04:51:19,065 - mmseg - INFO - Iter [6450/20000]	lr: 4.065e-05, eta: 10:11:21, time: 2.707, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0021, decode.acc_seg: 36.3390, loss: 0.0021
2023-05-27 04:53:34,127 - mmseg - INFO - Iter [6500/20000]	lr: 4.050e-05, eta: 10:09:05, time: 2.701, data_time: 2.442, memory: 3386, decode.loss_ce: 0.0015, decode.acc_seg: 35.2983, loss: 0.0015
2023-05-27 04:55:49,317 - mmseg - INFO - Iter [6550/20000]	lr: 4.035e-05, eta: 10:06:49, time: 2.704, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0011, decode.acc_seg: 38.0249, loss: 0.0011
2023-05-27 04:58:04,429 - mmseg - INFO - Iter [6600/20000]	lr: 4.020e-05, eta: 10:04:33, time: 2.702, data_time: 2.443, memory: 3386, decode.loss_ce: 0.0018, decode.acc_seg: 37.3600, loss: 0.0018
2023-05-27 05:00:19,666 - mmseg - INFO - Iter [6650/20000]	lr: 4.005e-05, eta: 10:02:18, time: 2.705, data_time: 2.444, memory: 3386, decode.loss_ce: 0.0015, decode.acc_seg: 35.3464, loss: 0.0015
2023-05-27 05:02:34,790 - mmseg - INFO - Iter [6700/20000]	lr: 3.990e-05, eta: 10:00:02, time: 2.702, data_time: 2.443, memory: 3386, decode.loss_ce: 0.0027, decode.acc_seg: 37.6173, loss: 0.0027
2023-05-27 05:04:49,989 - mmseg - INFO - Iter [6750/20000]	lr: 3.975e-05, eta: 9:57:46, time: 2.704, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0032, decode.acc_seg: 37.4843, loss: 0.0032
2023-05-27 05:07:05,399 - mmseg - INFO - Iter [6800/20000]	lr: 3.960e-05, eta: 9:55:31, time: 2.708, data_time: 2.447, memory: 3386, decode.loss_ce: 0.0019, decode.acc_seg: 37.5431, loss: 0.0019
2023-05-27 05:09:20,734 - mmseg - INFO - Iter [6850/20000]	lr: 3.945e-05, eta: 9:53:16, time: 2.707, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0017, decode.acc_seg: 38.0185, loss: 0.0017
2023-05-27 05:11:36,061 - mmseg - INFO - Iter [6900/20000]	lr: 3.930e-05, eta: 9:51:00, time: 2.707, data_time: 2.446, memory: 3386, decode.loss_ce: 0.0015, decode.acc_seg: 37.5556, loss: 0.0015
2023-05-27 05:13:51,405 - mmseg - INFO - Iter [6950/20000]	lr: 3.915e-05, eta: 9:48:45, time: 2.707, data_time: 2.446, memory: 3386, decode.loss_ce: 0.0015, decode.acc_seg: 38.0176, loss: 0.0015
2023-05-27 05:16:06,715 - mmseg - INFO - Exp name: segformer_mit-b5_512x512_20k_safeforest_over_canopy_compressed.py
2023-05-27 05:16:06,721 - mmseg - INFO - Iter [7000/20000]	lr: 3.900e-05, eta: 9:46:30, time: 2.706, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0027, decode.acc_seg: 37.3308, loss: 0.0027
2023-05-27 05:18:22,070 - mmseg - INFO - Iter [7050/20000]	lr: 3.885e-05, eta: 9:44:14, time: 2.707, data_time: 2.446, memory: 3386, decode.loss_ce: 0.0040, decode.acc_seg: 38.3373, loss: 0.0040
2023-05-27 05:20:37,280 - mmseg - INFO - Iter [7100/20000]	lr: 3.870e-05, eta: 9:41:59, time: 2.704, data_time: 2.444, memory: 3386, decode.loss_ce: 0.0020, decode.acc_seg: 37.6130, loss: 0.0020
2023-05-27 05:22:52,530 - mmseg - INFO - Iter [7150/20000]	lr: 3.855e-05, eta: 9:39:43, time: 2.705, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0018, decode.acc_seg: 36.9048, loss: 0.0018
2023-05-27 05:25:07,682 - mmseg - INFO - Iter [7200/20000]	lr: 3.840e-05, eta: 9:37:27, time: 2.703, data_time: 2.442, memory: 3386, decode.loss_ce: 0.0013, decode.acc_seg: 35.9773, loss: 0.0013
2023-05-27 05:27:23,127 - mmseg - INFO - Iter [7250/20000]	lr: 3.825e-05, eta: 9:35:12, time: 2.709, data_time: 2.448, memory: 3386, decode.loss_ce: 0.0030, decode.acc_seg: 37.7580, loss: 0.0030
2023-05-27 05:29:38,354 - mmseg - INFO - Iter [7300/20000]	lr: 3.810e-05, eta: 9:32:57, time: 2.705, data_time: 2.444, memory: 3386, decode.loss_ce: 0.0022, decode.acc_seg: 37.8163, loss: 0.0022
2023-05-27 05:31:53,368 - mmseg - INFO - Iter [7350/20000]	lr: 3.795e-05, eta: 9:30:41, time: 2.700, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0018, decode.acc_seg: 37.7934, loss: 0.0018
2023-05-27 05:34:08,349 - mmseg - INFO - Iter [7400/20000]	lr: 3.780e-05, eta: 9:28:25, time: 2.700, data_time: 2.443, memory: 3386, decode.loss_ce: 0.0017, decode.acc_seg: 35.7364, loss: 0.0017
2023-05-27 05:36:23,479 - mmseg - INFO - Iter [7450/20000]	lr: 3.765e-05, eta: 9:26:09, time: 2.703, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0021, decode.acc_seg: 37.6923, loss: 0.0021
2023-05-27 05:38:38,773 - mmseg - INFO - Iter [7500/20000]	lr: 3.750e-05, eta: 9:23:54, time: 2.706, data_time: 2.446, memory: 3386, decode.loss_ce: 0.0009, decode.acc_seg: 38.4915, loss: 0.0009
2023-05-27 05:40:54,016 - mmseg - INFO - Iter [7550/20000]	lr: 3.735e-05, eta: 9:21:38, time: 2.705, data_time: 2.444, memory: 3386, decode.loss_ce: 0.0011, decode.acc_seg: 35.4011, loss: 0.0011
2023-05-27 05:43:09,265 - mmseg - INFO - Iter [7600/20000]	lr: 3.720e-05, eta: 9:19:23, time: 2.705, data_time: 2.446, memory: 3386, decode.loss_ce: 0.0012, decode.acc_seg: 37.8543, loss: 0.0012
2023-05-27 05:45:24,279 - mmseg - INFO - Iter [7650/20000]	lr: 3.705e-05, eta: 9:17:07, time: 2.700, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0018, decode.acc_seg: 37.9576, loss: 0.0018
2023-05-27 05:47:39,474 - mmseg - INFO - Iter [7700/20000]	lr: 3.690e-05, eta: 9:14:51, time: 2.704, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0116, decode.acc_seg: 35.5522, loss: 0.0116
2023-05-27 05:49:54,605 - mmseg - INFO - Iter [7750/20000]	lr: 3.675e-05, eta: 9:12:36, time: 2.703, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0184, decode.acc_seg: 37.1584, loss: 0.0184
2023-05-27 05:52:09,830 - mmseg - INFO - Iter [7800/20000]	lr: 3.660e-05, eta: 9:10:20, time: 2.705, data_time: 2.446, memory: 3386, decode.loss_ce: 0.0058, decode.acc_seg: 37.1152, loss: 0.0058
2023-05-27 05:54:25,177 - mmseg - INFO - Iter [7850/20000]	lr: 3.645e-05, eta: 9:08:05, time: 2.707, data_time: 2.446, memory: 3386, decode.loss_ce: 0.0022, decode.acc_seg: 36.2870, loss: 0.0022
2023-05-27 05:56:40,468 - mmseg - INFO - Iter [7900/20000]	lr: 3.630e-05, eta: 9:05:49, time: 2.706, data_time: 2.445, memory: 3386, decode.loss_ce: 0.0023, decode.acc_seg: 35.2218, loss: 0.0023
